This file is automatically generated by assertExpectedJournal calls in test_indexing.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestIndexing.test_arange)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_arange(out, length, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(length):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < length
    # src[test_indexing.py:N]: out[tile] = tile.index
    tl.store(out + indices_0 * 1, indices_0, mask_0)

def arange(length: int, device: torch.device, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = torch.empty([length], dtype=torch.int32, device=device)
    out = torch.empty([length], dtype=torch.int32, device=device)
    # src[test_indexing.py:N]: for tile in hl.tile(length):
    _BLOCK_SIZE_0 = 32
    # src[test_indexing.py:N]: for tile in hl.tile(length):
    # src[test_indexing.py:N]:     out[tile] = tile.index
    _launcher(_helion_arange, (triton.cdiv(length, _BLOCK_SIZE_0),), out, length, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_arange_block_size_multiple)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_arange_block_size_mul(out, _BLOCK_SIZE_0: tl.constexpr, mul_2: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    # src[test_indexing.py:N]: tile.begin * 2, tile.begin * 2 + tile.block_size * 2
    mul = 2 * offset_0
    # src[test_indexing.py:N]: indices = hl.arange(
    # src[test_indexing.py:N]:     tile.begin * 2, tile.begin * 2 + tile.block_size * 2
    # src[test_indexing.py:N]: )
    indices = mul + tl.arange(0, mul_2)
    # src[test_indexing.py:N]: out[indices] = indices
    tl.store(out + indices * 1, indices, None)

def arange_block_size_mul(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = torch.zeros([x.size(0) * 2], dtype=torch.int32, device=x.device)
    out = torch.zeros([x.size(0) * 2], dtype=torch.int32, device=x.device)
    # src[test_indexing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_indexing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_indexing.py:N]:     indices = hl.arange(
    # src[test_indexing.py:N]:         tile.begin * 2, tile.begin * 2 + tile.block_size * 2
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_arange_block_size_mul, (triton.cdiv(64, _BLOCK_SIZE_0),), out, _BLOCK_SIZE_0, 2 * _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_arange_three_args_step)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_arange_three_args_step(out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(x.size(0) // 2):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_indexing.py:N]: start_idx = tile.begin * 2
    mul = 2 * offset_0
    # src[test_indexing.py:N]: out[tile] = torch.arange(start_idx, end_idx, step=2, device=x.device)
    iota = (mul + 2 * tl.arange(0, _BLOCK_SIZE_0)).to(tl.int64)
    v_0 = tl.cast(iota, tl.int32)
    tl.store(out + indices_0 * 1, v_0, None)

def arange_three_args_step(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = torch.zeros([x.size(0) // 2], dtype=torch.int32, device=x.device)
    out = torch.zeros([x.size(0) // 2], dtype=torch.int32, device=x.device)
    # src[test_indexing.py:N]: for tile in hl.tile(x.size(0) // 2):
    _BLOCK_SIZE_0 = 8
    # src[test_indexing.py:N]: for tile in hl.tile(x.size(0) // 2):
    # src[test_indexing.py:N]:     # Test the exact pattern requested: torch.arange(start, end, step=2, device=x.device)
    # src[test_indexing.py:N]:     start_idx = tile.begin * 2
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_arange_three_args_step, (triton.cdiv(32, _BLOCK_SIZE_0),), out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_broadcasting_block_ptr_indexing)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_broadcast_add_3d(x, bias1, bias2, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[test_indexing.py:N]: for tile_l, tile_m, tile_n in hl.tile([d0, d1, d2]):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(24, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    offset_2 = pid_2 * _BLOCK_SIZE_2
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    load = tl.load(tl.make_block_ptr(x, [16, 24, 32], [768, 32, 1], [offset_0, offset_1, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    load_1 = tl.broadcast_to(tl.load(tl.make_block_ptr(bias1, [1, 24, 32], [768, 32, 1], [0, offset_1, offset_2], [1, _BLOCK_SIZE_1, _BLOCK_SIZE_2], [2, 1, 0]), boundary_check=[1, 2], padding_option='zero'), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    v_0 = load + load_1
    # src[test_indexing.py:N]: + bias2[tile_l, tile_m, tile_n]
    load_2 = tl.broadcast_to(tl.load(tl.make_block_ptr(bias2, [16, 1, 32], [32, 32, 1], [offset_0, 0, offset_2], [_BLOCK_SIZE_0, 1, _BLOCK_SIZE_2], [2, 1, 0]), boundary_check=[0, 2], padding_option='zero'), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]: + bias2[tile_l, tile_m, tile_n]
    v_1 = v_0 + load_2
    # src[test_indexing.py:N]: out[tile_l, tile_m, tile_n] = (
    # src[test_indexing.py:N]:     x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]:     + bias1[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    tl.store(tl.make_block_ptr(out, [16, 24, 32], [768, 32, 1], [offset_0, offset_1, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2], [2, 1, 0]), v_1, boundary_check=[0, 1, 2])

def broadcast_add_3d(x: torch.Tensor, bias1: torch.Tensor, bias2: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: d0, d1, d2 = x.size()
    d0, d1, d2 = x.size()
    # src[test_indexing.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_indexing.py:N]: for tile_l, tile_m, tile_n in hl.tile([d0, d1, d2]):
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 8
    _BLOCK_SIZE_2 = 8
    # src[test_indexing.py:N]: for tile_l, tile_m, tile_n in hl.tile([d0, d1, d2]):
    # src[test_indexing.py:N]:     # bias1 has shape [1, d1, d2], bias2 has shape [d0, 1, d2]
    # src[test_indexing.py:N]:     out[tile_l, tile_m, tile_n] = (
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_broadcast_add_3d, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(24, _BLOCK_SIZE_1) * triton.cdiv(32, _BLOCK_SIZE_2),), x, bias1, bias2, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_broadcasting_pointer_indexing)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_broadcast_add_3d(x, bias1, bias2, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[test_indexing.py:N]: for tile_l, tile_m, tile_n in hl.tile([d0, d1, d2]):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(24, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    load = tl.load(x + (indices_0[:, None, None] * 768 + indices_1[None, :, None] * 32 + indices_2[None, None, :] * 1), None)
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    load_1 = tl.broadcast_to(tl.load(bias1 + (indices_1[None, :, None] * 32 + indices_2[None, None, :] * 1), None), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    v_0 = load + load_1
    # src[test_indexing.py:N]: + bias2[tile_l, tile_m, tile_n]
    load_2 = tl.broadcast_to(tl.load(bias2 + (indices_0[:, None, None] * 32 + indices_2[None, None, :] * 1), None), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]: + bias2[tile_l, tile_m, tile_n]
    v_1 = v_0 + load_2
    # src[test_indexing.py:N]: out[tile_l, tile_m, tile_n] = (
    # src[test_indexing.py:N]:     x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]:     + bias1[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    tl.store(out + (indices_0[:, None, None] * 768 + indices_1[None, :, None] * 32 + indices_2[None, None, :] * 1), v_1, None)

def broadcast_add_3d(x: torch.Tensor, bias1: torch.Tensor, bias2: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: d0, d1, d2 = x.size()
    d0, d1, d2 = x.size()
    # src[test_indexing.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_indexing.py:N]: for tile_l, tile_m, tile_n in hl.tile([d0, d1, d2]):
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 8
    _BLOCK_SIZE_2 = 8
    # src[test_indexing.py:N]: for tile_l, tile_m, tile_n in hl.tile([d0, d1, d2]):
    # src[test_indexing.py:N]:     # bias1 has shape [1, d1, d2], bias2 has shape [d0, 1, d2]
    # src[test_indexing.py:N]:     out[tile_l, tile_m, tile_n] = (
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_broadcast_add_3d, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(24, _BLOCK_SIZE_1) * triton.cdiv(32, _BLOCK_SIZE_2),), x, bias1, bias2, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_broadcasting_tensor_descriptor_indexing)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

# src[test_indexing.py:N]: def broadcast_add_3d(
# src[test_indexing.py:N]:     x: torch.Tensor, bias1: torch.Tensor, bias2: torch.Tensor
# src[test_indexing.py:N]: ) -> torch.Tensor:
# src[test_indexing.py:N-N]: ...
helion.runtime.set_triton_allocator()

@triton.jit
def _helion_broadcast_add_3d(x, bias1, bias2, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    x_desc = tl.make_tensor_descriptor(x, [16, 24, 32], [768, 32, 1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    bias1_desc = tl.make_tensor_descriptor(bias1, [1, 24, 32], [768, 32, 1], [1, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: + bias2[tile_l, tile_m, tile_n]
    bias2_desc = tl.make_tensor_descriptor(bias2, [16, 1, 32], [32, 32, 1], [_BLOCK_SIZE_0, 1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: out[tile_l, tile_m, tile_n] = (
    # src[test_indexing.py:N]:     x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]:     + bias1[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    out_desc = tl.make_tensor_descriptor(out, [16, 24, 32], [768, 32, 1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: for tile_l, tile_m, tile_n in hl.tile([d0, d1, d2]):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(24, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    offset_2 = pid_2 * _BLOCK_SIZE_2
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    load = x_desc.load([offset_0, offset_1, offset_2])
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    load_1 = tl.broadcast_to(bias1_desc.load([0, offset_1, offset_2]), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    v_0 = load + load_1
    # src[test_indexing.py:N]: + bias2[tile_l, tile_m, tile_n]
    load_2 = tl.broadcast_to(bias2_desc.load([offset_0, 0, offset_2]), [_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2])
    # src[test_indexing.py:N]: x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]: + bias1[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]: + bias2[tile_l, tile_m, tile_n]
    v_1 = v_0 + load_2
    # src[test_indexing.py:N]: out[tile_l, tile_m, tile_n] = (
    # src[test_indexing.py:N]:     x[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N]:     + bias1[tile_l, tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    out_desc.store([offset_0, offset_1, offset_2], v_1)

def broadcast_add_3d(x: torch.Tensor, bias1: torch.Tensor, bias2: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: d0, d1, d2 = x.size()
    d0, d1, d2 = x.size()
    # src[test_indexing.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_indexing.py:N]: for tile_l, tile_m, tile_n in hl.tile([d0, d1, d2]):
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 8
    _BLOCK_SIZE_2 = 8
    # src[test_indexing.py:N]: for tile_l, tile_m, tile_n in hl.tile([d0, d1, d2]):
    # src[test_indexing.py:N]:     # bias1 has shape [1, d1, d2], bias2 has shape [d0, 1, d2]
    # src[test_indexing.py:N]:     out[tile_l, tile_m, tile_n] = (
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_broadcast_add_3d, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(24, _BLOCK_SIZE_1) * triton.cdiv(32, _BLOCK_SIZE_2),), x, bias1, bias2, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_gather_2d_dim0)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test_gather(input_tensor, index_tensor, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_indexing.py:N]: for tile_k, tile_m in hl.tile([K, M]):
    num_blocks_0 = tl.cdiv(8, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[test_indexing.py:N]: input_tensor[:, tile_m], 0, index_tensor[tile_k, tile_m]
    load = tl.load(input_tensor + (indices_2[:, None] * 32 + indices_1[None, :] * 1), None)
    load_1 = tl.load(index_tensor + (indices_0[:, None] * 32 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: out[tile_k, tile_m] = torch.gather(
    # src[test_indexing.py:N]:     input_tensor[:, tile_m], 0, index_tensor[tile_k, tile_m]
    # src[test_indexing.py:N]: )
    gather_result = tl.gather(load, load_1.to(tl.int32), axis=0)
    tl.store(out + (indices_0[:, None] * 32 + indices_1[None, :] * 1), gather_result, None)

def test_gather(input_tensor: torch.Tensor, index_tensor: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: K = index_tensor.size(0)
    K = index_tensor.size(0)
    # src[test_indexing.py:N]: M = input_tensor.size(1)
    M = input_tensor.size(1)
    # src[test_indexing.py:N]: out = torch.empty(
    # src[test_indexing.py:N]:     [K, M], dtype=input_tensor.dtype, device=input_tensor.device
    # src[test_indexing.py:N]: )
    out = torch.empty([K, M], dtype=input_tensor.dtype, device=input_tensor.device)
    # src[test_indexing.py:N]: for tile_k, tile_m in hl.tile([K, M]):
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 8
    _RDIM_SIZE_2 = 16
    # src[test_indexing.py:N]: for tile_k, tile_m in hl.tile([K, M]):
    # src[test_indexing.py:N]:     # Input sliced on non-gather dim to match index's second dim
    # src[test_indexing.py:N]:     out[tile_k, tile_m] = torch.gather(
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_test_gather, (triton.cdiv(8, _BLOCK_SIZE_0) * triton.cdiv(32, _BLOCK_SIZE_1),), input_tensor, index_tensor, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_gather_2d_dim1)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test_gather(input_tensor, index_tensor, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr):
    # src[test_indexing.py:N]: for tile_n, tile_k in hl.tile([N, K]):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[test_indexing.py:N]: input_tensor[tile_n, :], 1, index_tensor[tile_n, tile_k]
    load = tl.load(input_tensor + (indices_0[:, None] * 32 + indices_2[None, :] * 1), None)
    load_1 = tl.load(index_tensor + (indices_0[:, None] * 8 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: out[tile_n, tile_k] = torch.gather(
    # src[test_indexing.py:N]:     input_tensor[tile_n, :], 1, index_tensor[tile_n, tile_k]
    # src[test_indexing.py:N]: )
    gather_result = tl.gather(load, load_1.to(tl.int32), axis=1)
    tl.store(out + (indices_0[:, None] * 8 + indices_1[None, :] * 1), gather_result, None)

def test_gather(input_tensor: torch.Tensor, index_tensor: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: N = input_tensor.size(0)
    N = input_tensor.size(0)
    # src[test_indexing.py:N]: K = index_tensor.size(1)
    K = index_tensor.size(1)
    # src[test_indexing.py:N]: out = torch.empty(
    # src[test_indexing.py:N]:     [N, K], dtype=input_tensor.dtype, device=input_tensor.device
    # src[test_indexing.py:N]: )
    out = torch.empty([N, K], dtype=input_tensor.dtype, device=input_tensor.device)
    # src[test_indexing.py:N]: for tile_n, tile_k in hl.tile([N, K]):
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _RDIM_SIZE_2 = 32
    # src[test_indexing.py:N]: for tile_n, tile_k in hl.tile([N, K]):
    # src[test_indexing.py:N]:     # Input sliced on non-gather dim to match index's first dim
    # src[test_indexing.py:N]:     out[tile_n, tile_k] = torch.gather(
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_test_gather, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(8, _BLOCK_SIZE_1),), input_tensor, index_tensor, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _RDIM_SIZE_2, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_indexed_store_mask_propagation)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_scatter_kernel(dy, indices, dx, dy_size_0, dx_stride_0, dx_stride_1, dy_stride_0, dy_stride_1, indices_stride_0, indices_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m in hl.tile(dy.shape[0]):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < dy_size_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_indexing.py:N]: dx[tile_m.index[:, None], indices[tile_m, :]] = dy[tile_m, :]
    load = tl.load(dy + (indices_0[:, None] * dy_stride_0 + indices_1[None, :] * dy_stride_1), mask_0[:, None], other=0)
    load_1 = indices_0[:, None]
    load_2 = tl.load(indices + (indices_0[:, None] * indices_stride_0 + indices_1[None, :] * indices_stride_1), mask_0[:, None], other=0)
    tl.store(dx + (load_1 * dx_stride_0 + load_2 * dx_stride_1), load, mask_0[:, None])

def scatter_kernel(dy: torch.Tensor, indices: torch.Tensor, input_shape: list[int], k: int, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: dx = dy.new_zeros(*input_shape)
    dx = dy.new_zeros(*input_shape)
    # src[test_indexing.py:N]: k = hl.specialize(k)
    k = 8
    # src[test_indexing.py:N]: dx = dx.reshape(-1, dx.shape[-1])
    dx = dx.reshape(-1, dx.shape[-1])
    # src[test_indexing.py:N]: dy = dy.reshape(-1, k)
    dy = dy.reshape(-1, k)
    # src[test_indexing.py:N]: indices = indices.reshape(-1, k)
    indices = indices.reshape(-1, k)
    # src[test_indexing.py:N]: for tile_m in hl.tile(dy.shape[0]):
    _BLOCK_SIZE_0 = 2
    _RDIM_SIZE_1 = 8
    # src[test_indexing.py:N]: for tile_m in hl.tile(dy.shape[0]):
    # src[test_indexing.py:N]:     # This pattern uses tile_m.index[:, None] as a 2D tensor subscript
    # src[test_indexing.py:N]:     # which should propagate the tile's mask to the store
    # src[test_indexing.py:N-N]: ...
    _RDIM_SIZE_2 = triton.next_power_of_2(_BLOCK_SIZE_0)
    _launcher(_helion_scatter_kernel, (triton.cdiv(dy.size(0), _BLOCK_SIZE_0),), dy, indices, dx, dy.size(0), dx.stride(0), dx.stride(1), dy.stride(0), dy.stride(1), indices.stride(0), indices.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return dx.view(input_shape)
    return dx.view(input_shape)

--- assertExpectedJournal(TestIndexing.test_indirect_indexing_2d_direct_gather)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test(col, B, val, C, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    num_blocks_0 = tl.cdiv(32, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[test_indexing.py:N]: for tile_k in hl.tile(K):
    # src[test_indexing.py:N]:     cols_2d = col[tile_m, tile_k]
    # src[test_indexing.py:N]:     B_slice = B[cols_2d[:, :, None], tile_n.index[None, None, :]]
    # src[test_indexing.py:N-N]: ...
    for offset_3 in tl.range(0, 16, _BLOCK_SIZE_2):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[test_indexing.py:N]: cols_2d = col[tile_m, tile_k]
        cols_2d = tl.load(col + (indices_0[:, None] * 16 + indices_3[None, :] * 1), None)
        # src[test_indexing.py:N]: B_slice = B[cols_2d[:, :, None], tile_n.index[None, None, :]]
        subscript = cols_2d[:, :, None]
        load_1 = indices_1[None, None, :]
        B_slice = tl.load(B + (subscript * 24 + load_1 * 1), None)
        # src[test_indexing.py:N]: vals_2d = val[tile_m, tile_k]
        vals_2d = tl.load(val + (indices_0[:, None] * 16 + indices_3[None, :] * 1), None)
        # src[test_indexing.py:N]: contrib = vals_2d[:, :, None] * B_slice
        subscript_1 = vals_2d[:, :, None]
        v_0 = subscript_1 * B_slice
        # src[test_indexing.py:N]: contrib = contrib.sum(dim=1)
        contrib_1 = tl.cast(tl.sum(v_0, 1), tl.float32)
        # src[test_indexing.py:N]: acc = acc + contrib
        acc = acc_copy_0 + contrib_1
    # src[test_indexing.py:N]: C[tile_m, tile_n] = acc.to(out_dtype)
    tl.store(C + (indices_0[:, None] * 24 + indices_1[None, :] * 1), acc, None)

def test(col: torch.Tensor, val: torch.Tensor, B: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: M, K = col.shape
    M, K = col.shape
    # src[test_indexing.py:N]: _, N = B.shape
    _, N = B.shape
    # src[test_indexing.py:N]: out_dtype = torch.promote_types(val.dtype, B.dtype)
    out_dtype = torch.promote_types(val.dtype, B.dtype)
    # src[test_indexing.py:N]: C = torch.empty((M, N), dtype=out_dtype, device=B.device)
    C = torch.empty((M, N), dtype=out_dtype, device=B.device)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 8
    # src[test_indexing.py:N]: for tile_k in hl.tile(K):
    # src[test_indexing.py:N]:     cols_2d = col[tile_m, tile_k]
    # src[test_indexing.py:N]:     B_slice = B[cols_2d[:, :, None], tile_n.index[None, None, :]]
    # src[test_indexing.py:N-N]: ...
    _BLOCK_SIZE_2 = 4
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    # src[test_indexing.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[test_indexing.py:N-N]: ...
    _RDIM_SIZE_3 = triton.next_power_of_2(_BLOCK_SIZE_1)
    _launcher(_helion_test, (triton.cdiv(32, _BLOCK_SIZE_0) * triton.cdiv(24, _BLOCK_SIZE_1),), col, B, val, C, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return C
    return C

--- assertExpectedJournal(TestIndexing.test_indirect_indexing_2d_flat_load)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test(col, B_flat, val, C, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    num_blocks_0 = tl.cdiv(32, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    # src[test_indexing.py:N]: for tile_k in hl.tile(K):
    # src[test_indexing.py:N]:     cols_2d = col[tile_m, tile_k]
    # src[test_indexing.py:N]:     B_indices = (cols_2d * N)[:, :, None] + tile_n.index[None, None, :]
    # src[test_indexing.py:N-N]: ...
    for offset_3 in tl.range(0, 16, _BLOCK_SIZE_2):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[test_indexing.py:N]: cols_2d = col[tile_m, tile_k]
        cols_2d = tl.load(col + (indices_0[:, None] * 16 + indices_3[None, :] * 1), None)
        # src[test_indexing.py:N]: B_indices = (cols_2d * N)[:, :, None] + tile_n.index[None, None, :]
        v_0 = tl.full([], 24, tl.int64)
        v_1 = tl.cast(cols_2d * v_0, tl.int64)
        subscript = v_1[:, :, None]
        load_1 = indices_1[None, None, :]
        v_2 = tl.cast(load_1, tl.int64)
        v_3 = subscript + v_2
        # src[test_indexing.py:N]: B_slice = hl.load(B_flat, [B_indices])
        B_slice = tl.load(B_flat + v_3 * 1, None)
        # src[test_indexing.py:N]: vals_2d = val[tile_m, tile_k]
        vals_2d = tl.load(val + (indices_0[:, None] * 16 + indices_3[None, :] * 1), None)
        # src[test_indexing.py:N]: contrib = vals_2d[:, :, None] * B_slice
        subscript_1 = vals_2d[:, :, None]
        v_4 = subscript_1 * B_slice
        # src[test_indexing.py:N]: contrib = contrib.sum(dim=1)
        contrib_1 = tl.cast(tl.sum(v_4, 1), tl.float32)
        # src[test_indexing.py:N]: acc = acc + contrib
        acc = acc_copy_0 + contrib_1
    # src[test_indexing.py:N]: C[tile_m, tile_n] = acc.to(out_dtype)
    tl.store(C + (indices_0[:, None] * 24 + indices_1[None, :] * 1), acc, None)

def test(col: torch.Tensor, val: torch.Tensor, B: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: M, K = col.shape
    M, K = col.shape
    # src[test_indexing.py:N]: _, N = B.shape
    _, N = B.shape
    # src[test_indexing.py:N]: out_dtype = torch.promote_types(val.dtype, B.dtype)
    out_dtype = torch.promote_types(val.dtype, B.dtype)
    # src[test_indexing.py:N]: C = torch.empty((M, N), dtype=out_dtype, device=B.device)
    C = torch.empty((M, N), dtype=out_dtype, device=B.device)
    # src[test_indexing.py:N]: B_flat = B.reshape(-1)  # [K*N]
    B_flat = B.reshape(-1)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 8
    # src[test_indexing.py:N]: for tile_k in hl.tile(K):
    # src[test_indexing.py:N]:     cols_2d = col[tile_m, tile_k]
    # src[test_indexing.py:N]:     B_indices = (cols_2d * N)[:, :, None] + tile_n.index[None, None, :]
    # src[test_indexing.py:N-N]: ...
    _BLOCK_SIZE_2 = 4
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([M, N]):
    # src[test_indexing.py:N]:     acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
    # src[test_indexing.py:N-N]: ...
    _RDIM_SIZE_3 = triton.next_power_of_2(_BLOCK_SIZE_1)
    _launcher(_helion_test, (triton.cdiv(32, _BLOCK_SIZE_0) * triton.cdiv(24, _BLOCK_SIZE_1),), col, B_flat, val, C, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return C
    return C

--- assertExpectedJournal(TestIndexing.test_indirect_indexing_3d_direct_gather)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test(col, B, val, C, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr, _BLOCK_SIZE_4: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n, tile_p, tile_q in hl.tile([M, N, P, Q]):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(12, _BLOCK_SIZE_1)
    num_blocks_2 = tl.cdiv(10, _BLOCK_SIZE_2)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1) % num_blocks_2
    pid_3 = tl.program_id(0) // (num_blocks_0 * num_blocks_1 * num_blocks_2)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    mask_2 = indices_2 < 10
    offset_3 = pid_3 * _BLOCK_SIZE_3
    indices_3 = (offset_3 + tl.arange(0, _BLOCK_SIZE_3)).to(tl.int32)
    mask_3 = indices_3 < 14
    # src[test_indexing.py:N]: acc = hl.zeros([tile_m, tile_n, tile_p, tile_q], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3], 0.0, tl.float32)
    # src[test_indexing.py:N]: for tile_k in hl.tile(K):
    # src[test_indexing.py:N]:     cols_3d = col[tile_m, tile_n, tile_k]
    # src[test_indexing.py:N]:     B_slice = B[
    # src[test_indexing.py:N-N]: ...
    for offset_7 in tl.range(0, 8, _BLOCK_SIZE_4):
        indices_7 = offset_7 + tl.arange(0, _BLOCK_SIZE_4).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[test_indexing.py:N]: cols_3d = col[tile_m, tile_n, tile_k]
        cols_3d = tl.load(col + (indices_0[:, None, None] * 96 + indices_1[None, :, None] * 8 + indices_7[None, None, :] * 1), None)
        # src[test_indexing.py:N]: cols_3d[:, :, :, None, None],
        subscript = cols_3d[:, :, :, None, None]
        # src[test_indexing.py:N]: tile_p.index[None, None, :, None],
        load_1 = indices_2[None, None, :, None]
        # src[test_indexing.py:N]: tile_q.index[None, None, None, :],
        load_2 = indices_3[None, None, None, :]
        # src[test_indexing.py:N]: B_slice = B[
        # src[test_indexing.py:N]:     cols_3d[:, :, :, None, None],
        # src[test_indexing.py:N]:     tile_p.index[None, None, :, None],
        # src[test_indexing.py:N-N]: ...
        B_slice = tl.load(B + (subscript * 140 + load_1 * 14 + load_2 * 1), mask_2[None, None, None, :, None] & mask_3[None, None, None, None, :], other=0)
        # src[test_indexing.py:N]: vals_3d = val[tile_m, tile_n, tile_k]
        vals_3d = tl.load(val + (indices_0[:, None, None] * 96 + indices_1[None, :, None] * 8 + indices_7[None, None, :] * 1), None)
        # src[test_indexing.py:N]: contrib = vals_3d[:, :, :, None, None] * B_slice
        subscript_1 = vals_3d[:, :, :, None, None]
        v_0 = subscript_1 * B_slice
        # src[test_indexing.py:N]: contrib = contrib.sum(dim=2)
        contrib_1 = tl.cast(tl.sum(v_0, 2), tl.float32)
        # src[test_indexing.py:N]: acc = acc + contrib
        acc = acc_copy_0 + contrib_1
    # src[test_indexing.py:N]: C[tile_m, tile_n, tile_p, tile_q] = acc.to(out_dtype)
    tl.store(C + (indices_0[:, None, None, None] * 1680 + indices_1[None, :, None, None] * 140 + indices_2[None, None, :, None] * 14 + indices_3[None, None, None, :] * 1), acc, mask_2[None, None, :, None] & mask_3[None, None, None, :])

def test(col: torch.Tensor, val: torch.Tensor, B: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: M, N, K = col.shape
    M, N, K = col.shape
    # src[test_indexing.py:N]: _, P, Q = B.shape
    _, P, Q = B.shape
    # src[test_indexing.py:N]: out_dtype = torch.promote_types(val.dtype, B.dtype)
    out_dtype = torch.promote_types(val.dtype, B.dtype)
    # src[test_indexing.py:N]: C = torch.empty((M, N, P, Q), dtype=out_dtype, device=B.device)
    C = torch.empty((M, N, P, Q), dtype=out_dtype, device=B.device)
    # src[test_indexing.py:N]: for tile_m, tile_n, tile_p, tile_q in hl.tile([M, N, P, Q]):
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 4
    _BLOCK_SIZE_3 = 4
    # src[test_indexing.py:N]: for tile_k in hl.tile(K):
    # src[test_indexing.py:N]:     cols_3d = col[tile_m, tile_n, tile_k]
    # src[test_indexing.py:N]:     B_slice = B[
    # src[test_indexing.py:N-N]: ...
    _BLOCK_SIZE_4 = 4
    # src[test_indexing.py:N]: for tile_m, tile_n, tile_p, tile_q in hl.tile([M, N, P, Q]):
    # src[test_indexing.py:N]:     acc = hl.zeros([tile_m, tile_n, tile_p, tile_q], dtype=torch.float32)
    # src[test_indexing.py:N-N]: ...
    _RDIM_SIZE_5 = triton.next_power_of_2(_BLOCK_SIZE_2)
    _RDIM_SIZE_6 = triton.next_power_of_2(_BLOCK_SIZE_3)
    _launcher(_helion_test, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(12, _BLOCK_SIZE_1) * triton.cdiv(10, _BLOCK_SIZE_2) * triton.cdiv(14, _BLOCK_SIZE_3),), col, B, val, C, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, _BLOCK_SIZE_4, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return C
    return C

--- assertExpectedJournal(TestIndexing.test_indirect_indexing_3d_flat_load)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test(col, B_flat, val, C, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr, _BLOCK_SIZE_4: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n, tile_p, tile_q in hl.tile([M, N, P, Q]):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(12, _BLOCK_SIZE_1)
    num_blocks_2 = tl.cdiv(10, _BLOCK_SIZE_2)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1) % num_blocks_2
    pid_3 = tl.program_id(0) // (num_blocks_0 * num_blocks_1 * num_blocks_2)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    mask_2 = indices_2 < 10
    offset_3 = pid_3 * _BLOCK_SIZE_3
    indices_3 = (offset_3 + tl.arange(0, _BLOCK_SIZE_3)).to(tl.int32)
    mask_3 = indices_3 < 14
    # src[test_indexing.py:N]: acc = hl.zeros([tile_m, tile_n, tile_p, tile_q], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3], 0.0, tl.float32)
    # src[test_indexing.py:N]: for tile_k in hl.tile(K):
    # src[test_indexing.py:N]:     cols_3d = col[tile_m, tile_n, tile_k]
    # src[test_indexing.py:N]:     B_indices = (
    # src[test_indexing.py:N-N]: ...
    for offset_7 in tl.range(0, 8, _BLOCK_SIZE_4):
        indices_7 = offset_7 + tl.arange(0, _BLOCK_SIZE_4).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[test_indexing.py:N]: cols_3d = col[tile_m, tile_n, tile_k]
        cols_3d = tl.load(col + (indices_0[:, None, None] * 96 + indices_1[None, :, None] * 8 + indices_7[None, None, :] * 1), None)
        # src[test_indexing.py:N]: cols_3d[:, :, :, None, None] * (P * Q)
        subscript = cols_3d[:, :, :, None, None]
        v_0 = tl.full([], 140, tl.int64)
        v_1 = tl.cast(subscript * v_0, tl.int64)
        # src[test_indexing.py:N]: + tile_p.index[None, None, :, None] * Q
        load_1 = indices_2[None, None, :, None]
        v_2 = tl.full([], 14, tl.int32)
        v_3 = tl.cast(load_1 * v_2, tl.int32)
        # src[test_indexing.py:N]: cols_3d[:, :, :, None, None] * (P * Q)
        # src[test_indexing.py:N]: + tile_p.index[None, None, :, None] * Q
        v_4 = v_3[None, :, :, :, :]
        v_5 = tl.cast(v_4, tl.int64)
        v_6 = v_1 + v_5
        # src[test_indexing.py:N]: + tile_q.index[None, None, None, :]
        load_2 = indices_3[None, None, None, :]
        # src[test_indexing.py:N]: cols_3d[:, :, :, None, None] * (P * Q)
        # src[test_indexing.py:N]: + tile_p.index[None, None, :, None] * Q
        # src[test_indexing.py:N]: + tile_q.index[None, None, None, :]
        v_7 = load_2[None, :, :, :, :]
        v_8 = tl.cast(v_7, tl.int64)
        v_9 = v_6 + v_8
        # src[test_indexing.py:N]: B_slice = hl.load(B_flat, [B_indices])
        B_slice = tl.load(B_flat + v_9 * 1, mask_2[None, None, None, :, None] & mask_3[None, None, None, None, :], other=0)
        # src[test_indexing.py:N]: vals_3d = val[tile_m, tile_n, tile_k]
        vals_3d = tl.load(val + (indices_0[:, None, None] * 96 + indices_1[None, :, None] * 8 + indices_7[None, None, :] * 1), None)
        # src[test_indexing.py:N]: contrib = vals_3d[:, :, :, None, None] * B_slice
        subscript_1 = vals_3d[:, :, :, None, None]
        v_10 = subscript_1 * B_slice
        # src[test_indexing.py:N]: contrib = contrib.sum(dim=2)
        contrib_1 = tl.cast(tl.sum(v_10, 2), tl.float32)
        # src[test_indexing.py:N]: acc = acc + contrib
        acc = acc_copy_0 + contrib_1
    # src[test_indexing.py:N]: C[tile_m, tile_n, tile_p, tile_q] = acc.to(out_dtype)
    tl.store(C + (indices_0[:, None, None, None] * 1680 + indices_1[None, :, None, None] * 140 + indices_2[None, None, :, None] * 14 + indices_3[None, None, None, :] * 1), acc, mask_2[None, None, :, None] & mask_3[None, None, None, :])

def test(col: torch.Tensor, val: torch.Tensor, B: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: M, N, K = col.shape
    M, N, K = col.shape
    # src[test_indexing.py:N]: _, P, Q = B.shape
    _, P, Q = B.shape
    # src[test_indexing.py:N]: out_dtype = torch.promote_types(val.dtype, B.dtype)
    out_dtype = torch.promote_types(val.dtype, B.dtype)
    # src[test_indexing.py:N]: C = torch.empty((M, N, P, Q), dtype=out_dtype, device=B.device)
    C = torch.empty((M, N, P, Q), dtype=out_dtype, device=B.device)
    # src[test_indexing.py:N]: B_flat = B.reshape(-1)  # [K*P*Q]
    B_flat = B.reshape(-1)
    # src[test_indexing.py:N]: for tile_m, tile_n, tile_p, tile_q in hl.tile([M, N, P, Q]):
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_1 = 4
    _BLOCK_SIZE_2 = 4
    _BLOCK_SIZE_3 = 4
    # src[test_indexing.py:N]: for tile_k in hl.tile(K):
    # src[test_indexing.py:N]:     cols_3d = col[tile_m, tile_n, tile_k]
    # src[test_indexing.py:N]:     B_indices = (
    # src[test_indexing.py:N-N]: ...
    _BLOCK_SIZE_4 = 4
    # src[test_indexing.py:N]: for tile_m, tile_n, tile_p, tile_q in hl.tile([M, N, P, Q]):
    # src[test_indexing.py:N]:     acc = hl.zeros([tile_m, tile_n, tile_p, tile_q], dtype=torch.float32)
    # src[test_indexing.py:N-N]: ...
    _RDIM_SIZE_5 = triton.next_power_of_2(_BLOCK_SIZE_2)
    _RDIM_SIZE_6 = triton.next_power_of_2(_BLOCK_SIZE_3)
    _launcher(_helion_test, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(12, _BLOCK_SIZE_1) * triton.cdiv(10, _BLOCK_SIZE_2) * triton.cdiv(14, _BLOCK_SIZE_3),), col, B_flat, val, C, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, _BLOCK_SIZE_4, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return C
    return C

--- assertExpectedJournal(TestIndexing.test_mask_load)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_masked_load(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 200
    # src[test_indexing.py:N]: out[tile] = hl.load(x, [tile], extra_mask=(tile.index % 2) == 0)
    v_0 = tl.full([], 2, tl.int32)
    v_1 = indices_0 % v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = v_1 != v_2
    v_4 = libdevice.signbit(v_1) != 0 if v_1.dtype is tl.float32 else v_1 < 0
    v_5 = libdevice.signbit(v_0) != 0 if v_0.dtype is tl.float32 else v_0 < 0
    v_6 = v_4 != v_5
    v_7 = v_3 & v_6
    v_8 = v_1 + v_0
    v_9 = tl.where(v_7, v_8, v_1)
    v_10 = tl.full([], 0, tl.int32)
    v_11 = v_9 == v_10
    load = tl.load(x + indices_0 * 1, mask_0 & v_11, other=0)
    tl.store(out + indices_0 * 1, load, mask_0)

def masked_load(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = torch.zeros_like(x)
    out = torch.zeros_like(x)
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    _BLOCK_SIZE_0 = 16
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    # src[test_indexing.py:N]:     out[tile] = hl.load(x, [tile], extra_mask=(tile.index % 2) == 0)
    _launcher(_helion_masked_load, (triton.cdiv(200, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_mask_store)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_masked_store(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 200
    # src[test_indexing.py:N]: hl.store(out, [tile], x[tile], extra_mask=(tile.index % 2) == 0)
    load = tl.load(x + indices_0 * 1, mask_0, other=0)
    v_0 = tl.full([], 2, tl.int32)
    v_1 = indices_0 % v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = v_1 != v_2
    v_4 = libdevice.signbit(v_1) != 0 if v_1.dtype is tl.float32 else v_1 < 0
    v_5 = libdevice.signbit(v_0) != 0 if v_0.dtype is tl.float32 else v_0 < 0
    v_6 = v_4 != v_5
    v_7 = v_3 & v_6
    v_8 = v_1 + v_0
    v_9 = tl.where(v_7, v_8, v_1)
    v_10 = tl.full([], 0, tl.int32)
    v_11 = v_9 == v_10
    tl.store(out + indices_0 * 1, load, mask_0 & v_11)

def masked_store(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = torch.zeros_like(x)
    out = torch.zeros_like(x)
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    _BLOCK_SIZE_0 = 16
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    # src[test_indexing.py:N]:     hl.store(out, [tile], x[tile], extra_mask=(tile.index % 2) == 0)
    _launcher(_helion_masked_store, (triton.cdiv(200, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_mixed_scalar_block_store_size1_dim)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_kernel_with_mixed_store(x_data, out, scales, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for m_tile, n_tile in hl.tile([m, n], block_size=[None, n_block]):
    num_blocks_0 = 1
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_1 * _BLOCK_SIZE_0
    # src[test_indexing.py:N]: n_tile.begin, n_tile.end, block_size=BLOCK_SIZE
    tile_end = offset_0 + _BLOCK_SIZE_0
    # src[test_indexing.py:N]: for n_tile_local in hl.tile(
    # src[test_indexing.py:N]:     n_tile.begin, n_tile.end, block_size=BLOCK_SIZE
    # src[test_indexing.py:N]: ):
    # src[test_indexing.py:N-N]: ...
    for offset_2 in tl.range(offset_0.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        mask_2 = indices_2 < tile_end
        # src[test_indexing.py:N]: x_block = x_data[m_tile, n_tile_local]
        x_block = tl.broadcast_to(tl.load(x_data + indices_2[None, :] * 1, mask_2[None, :], other=0), [_BLOCK_SIZE_1, _BLOCK_SIZE_2])
        # src[test_indexing.py:N]: row_max = x_block.abs().amax(dim=1)
        v_0 = tl_math.abs(x_block)
        _mask_to = tl.where(tl.broadcast_to(mask_2[None, :], [_BLOCK_SIZE_1, _BLOCK_SIZE_2]), v_0, tl.full([], float('-inf'), tl.float32))
        row_max = tl.cast(tl.max(_mask_to, 1), tl.float32)
        # src[test_indexing.py:N]: row_value = row_max.to(torch.uint8)
        v_1 = tl.cast(row_max, tl.uint8)
        # src[test_indexing.py:N]: out[m_tile, n_tile_local] = x_block * 2.0
        v_2 = 2.0
        v_3 = x_block * v_2
        tl.store(out + tl.broadcast_to(indices_2[None, :] * 1, [_BLOCK_SIZE_1, _BLOCK_SIZE_2]), v_3, mask_2[None, :])
        # src[test_indexing.py:N]: scale_col_idx = n_tile_local.begin // BLOCK_SIZE  # scalar
        floordiv = triton_helpers.div_floor_integer(offset_2, 32)
        # src[test_indexing.py:N]: scales[m_tile, scale_col_idx] = row_value  # row_value is block
        tl.store(scales + tl.broadcast_to(floordiv * 1, [_BLOCK_SIZE_1]), tl.reshape(v_1, []), None)

def kernel_with_mixed_store(x_data: torch.Tensor, BLOCK_SIZE: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = x_data.shape
    m, n = x_data.shape
    # src[test_indexing.py:N]: n = hl.specialize(n)
    n = 64
    # src[test_indexing.py:N]: n_scale_cols = (n + BLOCK_SIZE - 1) // BLOCK_SIZE
    n_scale_cols = (n + 32 - 1) // 32
    # src[test_indexing.py:N]: scales = x_data.new_empty((m, n_scale_cols), dtype=torch.uint8)
    scales = x_data.new_empty((m, n_scale_cols), dtype=torch.uint8)
    # src[test_indexing.py:N]: out = x_data.new_empty(x_data.shape, dtype=torch.float32)
    out = x_data.new_empty(x_data.shape, dtype=torch.float32)
    # src[test_indexing.py:N]: for m_tile, n_tile in hl.tile([m, n], block_size=[None, n_block]):
    _BLOCK_SIZE_0 = 32
    # src[test_indexing.py:N]: for n_tile_local in hl.tile(
    # src[test_indexing.py:N]:     n_tile.begin, n_tile.end, block_size=BLOCK_SIZE
    # src[test_indexing.py:N]: ):
    # src[test_indexing.py:N-N]: ...
    _BLOCK_SIZE_2 = 32
    # src[test_indexing.py:N]: x_block = x_data[m_tile, n_tile_local]
    _BLOCK_SIZE_1 = 1
    # src[test_indexing.py:N]: for m_tile, n_tile in hl.tile([m, n], block_size=[None, n_block]):
    # src[test_indexing.py:N]:     for n_tile_local in hl.tile(
    # src[test_indexing.py:N]:         n_tile.begin, n_tile.end, block_size=BLOCK_SIZE
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_kernel_with_mixed_store, (1 * triton.cdiv(64, _BLOCK_SIZE_0),), x_data, out, scales, _BLOCK_SIZE_0, _BLOCK_SIZE_2, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out, scales
    return (out, scales)

--- assertExpectedJournal(TestIndexing.test_non_consecutive_tensor_indexers_no_broadcast)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_store_with_mixed_indices(data, tensor_idx, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m in hl.tile(m, block_size=4):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_indexing.py:N]: val = hl.load(data, [tile_m, hl.arange(k, dtype=torch.int32)])
    iota = tl.arange(0, 16)
    val = tl.load(data + (indices_0[:, None] * 16 + iota[None, :] * 1), None)
    # src[test_indexing.py:N]: val_3d = val[:, None, :].expand(val.size(0), val.size(0), k)
    subscript = val[:, None, :]
    val_3d = tl.broadcast_to(subscript, [_BLOCK_SIZE_0, _BLOCK_SIZE_0, 16])
    # src[test_indexing.py:N]: [tensor_idx[tile_m], tile_m.index, hl.arange(k, dtype=torch.int32)],
    load_1 = tl.load(tensor_idx + indices_0 * 1, None)
    iota_1 = tl.arange(0, 16)
    # src[test_indexing.py:N]: hl.store(
    # src[test_indexing.py:N]:     out,
    # src[test_indexing.py:N]:     [tensor_idx[tile_m], tile_m.index, hl.arange(k, dtype=torch.int32)],
    # src[test_indexing.py:N-N]: ...
    tl.store(out + (load_1[:, None, None] * 128 + indices_0[None, :, None] * 16 + iota_1[None, None, :] * 1), val_3d, None)

def store_with_mixed_indices(tensor_idx: torch.Tensor, data: torch.Tensor, k: int, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = data.size()
    m, n = data.size()
    # src[test_indexing.py:N]: k = hl.specialize(k)
    k = 16
    # src[test_indexing.py:N]: out = torch.zeros([m, m, k], device=data.device, dtype=data.dtype)
    out = torch.zeros([m, m, k], device=data.device, dtype=data.dtype)
    # src[test_indexing.py:N]: for tile_m in hl.tile(m, block_size=4):
    _BLOCK_SIZE_0 = 4
    # src[test_indexing.py:N]: for tile_m in hl.tile(m, block_size=4):
    # src[test_indexing.py:N]:     # Store 3D data into out[tensor_idx[tile_m], tile_m.index, :]
    # src[test_indexing.py:N]:     val = hl.load(data, [tile_m, hl.arange(k, dtype=torch.int32)])
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_store_with_mixed_indices, (triton.cdiv(8, _BLOCK_SIZE_0),), data, tensor_idx, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_pairwise_add)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pairwise_add(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 499
    # src[test_indexing.py:N]: out[tile] = x[tile] + x[tile.index + 1]
    load = tl.load(x + indices_0 * 1, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    load_1 = tl.load(x + (indices_0 + 1) * 1, mask_0, other=0)
    v_2 = load + load_1
    tl.store(out + indices_0 * 1, v_2, mask_0)

def pairwise_add(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = x.new_empty([x.size(0) - 1])
    out = x.new_empty([x.size(0) - 1])
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    # src[test_indexing.py:N]:     out[tile] = x[tile] + x[tile.index + 1]
    _launcher(_helion_pairwise_add, (triton.cdiv(499, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_pairwise_add_commuted_and_multi_offset)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_pairwise_add_variants(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 253
    # src[test_indexing.py:N]: left = x[1 + tile.index]
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    left = tl.load(x + (indices_0 + 1) * 1, mask_0, other=0)
    # src[test_indexing.py:N]: right = x[tile.index + 1 + 2]
    v_2 = tl.full([], 1, tl.int32)
    v_3 = indices_0 + v_2
    v_4 = tl.full([], 2, tl.int32)
    v_5 = v_3 + v_4
    right = tl.load(x + (indices_0 + 3) * 1, mask_0, other=0)
    # src[test_indexing.py:N]: out[tile] = left + right
    v_6 = left + right
    tl.store(out + indices_0 * 1, v_6, mask_0)

def pairwise_add_variants(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = x.new_empty([x.size(0) - 3])
    out = x.new_empty([x.size(0) - 3])
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    # src[test_indexing.py:N]:     left = x[1 + tile.index]
    # src[test_indexing.py:N]:     right = x[tile.index + 1 + 2]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_pairwise_add_variants, (triton.cdiv(253, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_per_load_and_store_indexing)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_load_store_kernel(a, b, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: val_a = a[tile_m, tile_n]
    val_a = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: val_b = b[tile_m, tile_n]
    val_b = tl.load(b + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: out[tile_m, tile_n] = val_a + val_b
    v_0 = val_a + val_b
    tl.store(tl.make_block_ptr(out, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def load_store_kernel(a: torch.Tensor, b: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = a.shape
    m, n = a.shape
    # src[test_indexing.py:N]: out = torch.empty_like(a)
    out = torch.empty_like(a)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_indexing.py:N]:     # 2 loads
    # src[test_indexing.py:N]:     val_a = a[tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_load_store_kernel, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), a, b, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_per_load_and_store_indexing)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_load_store_kernel(a, b, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: val_a = a[tile_m, tile_n]
    val_a = tl.load(tl.make_block_ptr(a, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    # src[test_indexing.py:N]: val_b = b[tile_m, tile_n]
    val_b = tl.load(tl.make_block_ptr(b, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    # src[test_indexing.py:N]: out[tile_m, tile_n] = val_a + val_b
    v_0 = val_a + val_b
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_0, None)

def load_store_kernel(a: torch.Tensor, b: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = a.shape
    m, n = a.shape
    # src[test_indexing.py:N]: out = torch.empty_like(a)
    out = torch.empty_like(a)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_indexing.py:N]:     # 2 loads
    # src[test_indexing.py:N]:     val_a = a[tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_load_store_kernel, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), a, b, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_per_load_and_store_indexing)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_load_store_kernel(a, b, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    # src[test_indexing.py:N]: val_a = a[tile_m, tile_n]
    val_a = tl.load(tl.make_block_ptr(a, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    # src[test_indexing.py:N]: val_b = b[tile_m, tile_n]
    val_b = tl.load(tl.make_block_ptr(b, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    # src[test_indexing.py:N]: out[tile_m, tile_n] = val_a + val_b
    v_0 = val_a + val_b
    tl.store(tl.make_block_ptr(out, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def load_store_kernel(a: torch.Tensor, b: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = a.shape
    m, n = a.shape
    # src[test_indexing.py:N]: out = torch.empty_like(a)
    out = torch.empty_like(a)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_indexing.py:N]:     # 2 loads
    # src[test_indexing.py:N]:     val_a = a[tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_load_store_kernel, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), a, b, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_per_load_and_store_indexing)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_load_store_kernel(a, b, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    # src[test_indexing.py:N]: val_a = a[tile_m, tile_n]
    val_a = tl.load(tl.make_block_ptr(a, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    # src[test_indexing.py:N]: val_b = b[tile_m, tile_n]
    val_b = tl.load(tl.make_block_ptr(b, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    # src[test_indexing.py:N]: out[tile_m, tile_n] = val_a + val_b
    v_0 = val_a + val_b
    tl.store(tl.make_block_ptr(out, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def load_store_kernel(a: torch.Tensor, b: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = a.shape
    m, n = a.shape
    # src[test_indexing.py:N]: out = torch.empty_like(a)
    out = torch.empty_like(a)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_indexing.py:N]:     # 2 loads
    # src[test_indexing.py:N]:     val_a = a[tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_load_store_kernel, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), a, b, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_per_load_indexing)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_multi_load_kernel(a, b, c, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: val_a = a[tile_m, tile_n]
    val_a = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: val_b = b[tile_m, tile_n]
    val_b = tl.load(b + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: val_c = c[tile_m, tile_n]
    val_c = tl.load(tl.make_block_ptr(c, [64, 64], [64, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    # src[test_indexing.py:N]: out[tile_m, tile_n] = val_a + val_b + val_c
    v_0 = val_a + val_b
    v_1 = v_0 + val_c
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_1, None)

def multi_load_kernel(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = a.shape
    m, n = a.shape
    # src[test_indexing.py:N]: out = torch.empty_like(a)
    out = torch.empty_like(a)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_indexing.py:N]:     val_a = a[tile_m, tile_n]
    # src[test_indexing.py:N]:     val_b = b[tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_multi_load_kernel, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), a, b, c, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_per_load_indexing_backward_compat)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_many_loads_kernel(a, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: v1 = a[tile_m, tile_n]
    v1 = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: v2 = a[tile_m, tile_n]
    v2 = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: v3 = a[tile_m, tile_n]
    v3 = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: out[tile_m, tile_n] = v1 + v2 + v3
    v_0 = v1 + v2
    v_1 = v_0 + v3
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_1, None)

def many_loads_kernel(a: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = a.shape
    m, n = a.shape
    # src[test_indexing.py:N]: out = torch.empty_like(a)
    out = torch.empty_like(a)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_indexing.py:N]:     v1 = a[tile_m, tile_n]
    # src[test_indexing.py:N]:     v2 = a[tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_many_loads_kernel, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), a, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_per_load_indexing_backward_compat)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_many_loads_kernel(a, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: v1 = a[tile_m, tile_n]
    v1 = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: v2 = a[tile_m, tile_n]
    v2 = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: v3 = a[tile_m, tile_n]
    v3 = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: out[tile_m, tile_n] = v1 + v2 + v3
    v_0 = v1 + v2
    v_1 = v_0 + v3
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_1, None)

def many_loads_kernel(a: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = a.shape
    m, n = a.shape
    # src[test_indexing.py:N]: out = torch.empty_like(a)
    out = torch.empty_like(a)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_indexing.py:N]:     v1 = a[tile_m, tile_n]
    # src[test_indexing.py:N]:     v2 = a[tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_many_loads_kernel, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), a, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_per_load_indexing_backward_compat)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_many_loads_kernel(a, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: v1 = a[tile_m, tile_n]
    v1 = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: v2 = a[tile_m, tile_n]
    v2 = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: v3 = a[tile_m, tile_n]
    v3 = tl.load(a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: out[tile_m, tile_n] = v1 + v2 + v3
    v_0 = v1 + v2
    v_1 = v_0 + v3
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_1, None)

def many_loads_kernel(a: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = a.shape
    m, n = a.shape
    # src[test_indexing.py:N]: out = torch.empty_like(a)
    out = torch.empty_like(a)
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    # src[test_indexing.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_indexing.py:N]:     v1 = a[tile_m, tile_n]
    # src[test_indexing.py:N]:     v2 = a[tile_m, tile_n]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_many_loads_kernel, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), a, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_size1_dimension_tile_reshape)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_size1_reshape_kernel(x, out, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile_1, tile_2 in hl.tile([x.size(0), x.size(1)]):
    num_blocks_0 = 1
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: block = x[tile_1, tile_2]
    block = tl.broadcast_to(tl.load(x + indices_1[None, :] * 1, None), [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    # src[test_indexing.py:N]: block_reshape = block.reshape([tile_1, tile_2])
    block_reshape = tl.reshape(block, [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    # src[test_indexing.py:N]: out[tile_1, tile_2] = block_reshape
    tl.store(out + tl.broadcast_to(indices_1[None, :] * 1, [_BLOCK_SIZE_0, _BLOCK_SIZE_1]), block_reshape, None)

def size1_reshape_kernel(x: torch.Tensor, out: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: for tile_1, tile_2 in hl.tile([x.size(0), x.size(1)]):
    _BLOCK_SIZE_1 = 16
    # src[test_indexing.py:N]: block = x[tile_1, tile_2]
    _BLOCK_SIZE_0 = 1
    # src[test_indexing.py:N]: for tile_1, tile_2 in hl.tile([x.size(0), x.size(1)]):
    # src[test_indexing.py:N]:     block = x[tile_1, tile_2]
    # src[test_indexing.py:N]:     # This reshape would fail before the fix when x.size(0) == 1
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_size1_reshape_kernel, (1 * triton.cdiv(16, _BLOCK_SIZE_1),), x, out, _BLOCK_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestIndexing.test_size1_dimension_variable_tile_range)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_variable_tile_range_kernel(query_start_lens, query, output, output_stride_1, query_stride_1, query_start_lens_stride_0, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _SHAPE_DIM_2: tl.constexpr, _SHAPE_DIM_5: tl.constexpr):
    # src[test_indexing.py:N]: for seq_tile in hl.tile(num_seqs, block_size=1):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_3 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[test_indexing.py:N]: query_start = query_start_lens[seq_idx]
    query_start = tl.load(query_start_lens + offset_0 * query_start_lens_stride_0, None)
    # src[test_indexing.py:N]: query_end = query_start_lens[seq_idx + 1]
    add = 1 + offset_0
    query_end = tl.load(query_start_lens + add * query_start_lens_stride_0, None)
    # src[test_indexing.py:N]: for tile_q in hl.tile(query_start, query_end):
    # src[test_indexing.py:N]:     q = query[tile_q, :]
    # src[test_indexing.py:N]:     q = q.reshape([tile_q.block_size, q_size_1])
    # src[test_indexing.py:N-N]: ...
    for offset_2 in tl.range(query_start.to(tl.int32), query_end.to(tl.int32), _BLOCK_SIZE_1):
        # src[test_indexing.py:N]: q = query[tile_q, :]
        q = tl.broadcast_to(tl.load(query + indices_3[None, :] * query_stride_1, None), [_BLOCK_SIZE_1, _SHAPE_DIM_2])
        # src[test_indexing.py:N]: q = q.reshape([tile_q.block_size, q_size_1])
        q_1 = tl.reshape(q, [_BLOCK_SIZE_1, 16])
        # src[test_indexing.py:N]: output[tile_q, :] = q
        tl.store(output + tl.broadcast_to(indices_3[None, :] * output_stride_1, [_BLOCK_SIZE_1, _SHAPE_DIM_5]), q_1, None)

def variable_tile_range_kernel(query: torch.Tensor, query_start_lens: torch.Tensor, num_seqs: int, output: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: for seq_tile in hl.tile(num_seqs, block_size=1):
    _RDIM_SIZE_2 = 16
    # src[test_indexing.py:N]: for tile_q in hl.tile(query_start, query_end):
    # src[test_indexing.py:N]:     q = query[tile_q, :]
    # src[test_indexing.py:N]:     q = q.reshape([tile_q.block_size, q_size_1])
    # src[test_indexing.py:N-N]: ...
    _BLOCK_SIZE_1 = 32
    # src[test_indexing.py:N]: q = query[tile_q, :]
    _SHAPE_DIM_2 = 16
    # src[test_indexing.py:N]: output[tile_q, :] = q
    _SHAPE_DIM_5 = 16
    # src[test_indexing.py:N]: for seq_tile in hl.tile(num_seqs, block_size=1):
    # src[test_indexing.py:N]:     seq_idx = seq_tile.begin
    # src[test_indexing.py:N]:     query_start = query_start_lens[seq_idx]
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_variable_tile_range_kernel, (num_seqs,), query_start_lens, query, output, output.stride(1), query.stride(1), query_start_lens.stride(0), _RDIM_SIZE_2, _BLOCK_SIZE_1, _SHAPE_DIM_2, _SHAPE_DIM_5, num_warps=4, num_stages=1)

--- assertExpectedJournal(TestIndexing.test_slice_block_size_multiple)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_arange_block_size_mul(ones, out, _BLOCK_SIZE_0: tl.constexpr, mul_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    # src[test_indexing.py:N]: indices_start = tile.begin * 2
    mul = 2 * offset_0
    # src[test_indexing.py:N]: out[indices_start:indices_end] = ones[indices_start:indices_end]
    iota = mul + tl.arange(0, mul_1)
    load = tl.load(ones + iota * 1, None)
    iota_1 = mul + tl.arange(0, mul_1)
    tl.store(out + iota_1 * 1, load, None)

def arange_block_size_mul(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = torch.zeros([x.size(0) * 2], dtype=torch.int32, device=x.device)
    out = torch.zeros([x.size(0) * 2], dtype=torch.int32, device=x.device)
    # src[test_indexing.py:N]: ones = torch.ones_like(out)
    ones = torch.ones_like(out)
    # src[test_indexing.py:N]: for tile in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_indexing.py:N]: for tile in hl.tile(x.size(0)):
    # src[test_indexing.py:N]:     indices_start = tile.begin * 2
    # src[test_indexing.py:N]:     indices_end = indices_start + tile.block_size * 2
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_arange_block_size_mul, (triton.cdiv(64, _BLOCK_SIZE_0),), ones, out, _BLOCK_SIZE_0, 2 * _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_tile_count_top_level)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fn(out, n, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(n, block_size=64):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    # src[test_indexing.py:N]: out[tile] = tile.count
    tile_count = tl.cdiv(n, _BLOCK_SIZE_0)
    tl.store(out + indices_0 * 1, tile_count, mask_0)

def fn(n: int, device: torch.device, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = torch.zeros([n], dtype=torch.int32, device=device)
    out = torch.zeros([n], dtype=torch.int32, device=device)
    # src[test_indexing.py:N]: for tile in hl.tile(n, block_size=64):
    _BLOCK_SIZE_0 = 64
    # src[test_indexing.py:N]: for tile in hl.tile(n, block_size=64):
    # src[test_indexing.py:N]:     out[tile] = tile.count
    _launcher(_helion_fn, (triton.cdiv(n, _BLOCK_SIZE_0),), out, n, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_tile_count_with_begin_end)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fn(out, begin, end, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: out[0] = tile.count
    tile_count = tl.cdiv(end + -1 * begin, _BLOCK_SIZE_0)
    tl.store(out + tl.zeros([], tl.int32), tile_count, None)

def fn(begin: int, end: int, device: torch.device, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = torch.zeros([1], dtype=torch.int32, device=device)
    out = torch.zeros([1], dtype=torch.int32, device=device)
    # src[test_indexing.py:N]: for tile in hl.tile(begin, end, block_size=32):
    _BLOCK_SIZE_0 = 32
    # src[test_indexing.py:N]: for tile in hl.tile(begin, end, block_size=32):
    # src[test_indexing.py:N]:     out[0] = tile.count
    _launcher(_helion_fn, (triton.cdiv(end + -1 * begin, _BLOCK_SIZE_0),), out, begin, end, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_tile_index_floor_div)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_dequant_with_scale(x_data, x_scale, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for m_tile, n_tile in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(128, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_indexing.py:N]: data = x_data[m_tile, n_tile]
    data = tl.load(x_data + (indices_0[:, None] * 256 + indices_1[None, :] * 1), None)
    # src[test_indexing.py:N]: scale = x_scale[m_tile, n_tile.index // block_size]
    v_0 = tl.full([], 32, tl.int32)
    v_1 = tl.where((indices_1 < 0) != (v_0 < 0), tl.where(indices_1 % v_0 != 0, indices_1 // v_0 - 1, indices_1 // v_0), indices_1 // v_0)
    scale = tl.load(x_scale + (indices_0[:, None] * 8 + v_1[None, :] * 1), None)
    # src[test_indexing.py:N]: out[m_tile, n_tile] = data * scale
    v_2 = data * scale
    tl.store(out + (indices_0[:, None] * 256 + indices_1[None, :] * 1), v_2, None)

def dequant_with_scale(x_data: torch.Tensor, x_scale: torch.Tensor, block_size: hl.constexpr, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: m, n = x_data.shape
    m, n = x_data.shape
    # src[test_indexing.py:N]: out = torch.empty_like(x_data)
    out = torch.empty_like(x_data)
    # src[test_indexing.py:N]: for m_tile, n_tile in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 64
    # src[test_indexing.py:N]: for m_tile, n_tile in hl.tile([m, n]):
    # src[test_indexing.py:N]:     data = x_data[m_tile, n_tile]
    # src[test_indexing.py:N]:     # Use floor division to index into scale
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_dequant_with_scale, (triton.cdiv(128, _BLOCK_SIZE_0) * triton.cdiv(256, _BLOCK_SIZE_1),), x_data, x_scale, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_tile_with_offset_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_tile_offset_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_indexing.py:N]: tile_offset = tile + 10
    v_0 = tl.full([], 10, tl.int32)
    v_1 = indices_0 + v_0
    # src[test_indexing.py:N]: out[tile] = x[tile_offset]
    load = tl.load(tl.make_block_ptr(x, [200], [1], [offset_0 + 10], [_BLOCK_SIZE_0], [0]), boundary_check=[0], padding_option='zero')
    tl.store(tl.make_block_ptr(out, [190], [1], [offset_0], [_BLOCK_SIZE_0], [0]), load, boundary_check=[0])

def tile_offset_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = x.new_empty(x.size(0) - 10)
    out = x.new_empty(x.size(0) - 10)
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    # src[test_indexing.py:N]:     # Use tile + offset pattern
    # src[test_indexing.py:N]:     tile_offset = tile + 10
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_tile_offset_kernel, (triton.cdiv(190, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_tile_with_offset_from_expr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q, k, v, lse, o, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: for tile_m in hl.tile(MM, block_size=block_m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_3 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[test_indexing.py:N]: m_i = hl.zeros([tile_m]) - float("inf")
    full = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    v_0 = float('inf')
    v_1 = full - v_0
    # src[test_indexing.py:N]: l_i = hl.zeros([tile_m]) + 1.0
    full_1 = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    v_2 = 1.0
    v_3 = full_1 + v_2
    # src[test_indexing.py:N]: acc = hl.zeros([tile_m, Dv])
    acc = tl.full([_BLOCK_SIZE_0, 64], 0.0, tl.float32)
    # src[test_indexing.py:N]: q_i = q[tile_m, :]
    q_i = tl.load(q + (indices_0[:, None] * 64 + indices_3[None, :] * 1), None)
    # src[test_indexing.py:N]: k_j = k[tile_n + start_N, :]
    symnode_0 = 64 * triton_helpers.div_floor_integer(offset_0, 64)
    # src[test_indexing.py:N]: for tile_n in hl.tile(0, N, block_size=block_n):
    # src[test_indexing.py:N]:     k_j = k[tile_n + start_N, :]
    # src[test_indexing.py:N]:     v_j = v[tile_n + start_N, :]
    # src[test_indexing.py:N-N]: ...
    for offset_2 in tl.range(0, 64, _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        q_i_copy = q_i
        v_1_copy = v_1
        acc_copy = acc
        v_3_copy = v_3
        q_i_copy_0 = q_i_copy
        v_1_copy_0 = v_1_copy
        acc_copy_0 = acc_copy
        v_3_copy_0 = v_3_copy
        # src[test_indexing.py:N]: k_j = k[tile_n + start_N, :]
        v_4 = tl.cast(symnode_0, tl.int32)
        v_5 = indices_2 + v_4
        k_j = tl.load(k + ((indices_2 + symnode_0)[:, None] * 64 + indices_3[None, :] * 1), None)
        # src[test_indexing.py:N]: v_j = v[tile_n + start_N, :]
        v_6 = tl.cast(symnode_0, tl.int32)
        v_7 = indices_2 + v_6
        v_j = tl.load(v + ((indices_2 + symnode_0)[:, None] * 64 + indices_3[None, :] * 1), None)
        # src[test_indexing.py:N]: qk = hl.dot(q_i, k_j.T, out_dtype=torch.float32)
        permute = tl.permute(k_j, [1, 0])
        qk = tl.dot(tl.cast(q_i_copy_0, tl.bfloat16), tl.cast(permute, tl.bfloat16), input_precision='tf32', out_dtype=tl.float32)
        # src[test_indexing.py:N]: m_ij = torch.maximum(m_i, torch.amax(qk, -1) * qk_scale)
        amax = tl.cast(tl.max(qk, 1), tl.float32)
        v_8 = 0.18033688
        v_9 = amax * v_8
        v_10 = triton_helpers.maximum(v_1_copy_0, v_9)
        # src[test_indexing.py:N]: qk = qk * qk_scale - m_ij[:, None]
        v_11 = 0.18033688
        v_12 = qk * v_11
        subscript = v_10[:, None]
        v_13 = v_12 - subscript
        # src[test_indexing.py:N]: p = torch.exp2(qk)
        v_14 = libdevice.exp2(v_13)
        # src[test_indexing.py:N]: alpha = torch.exp2(m_i - m_ij)
        v_15 = v_1_copy_0 - v_10
        v_16 = libdevice.exp2(v_15)
        # src[test_indexing.py:N]: l_ij = torch.sum(p, -1)
        l_ij = tl.cast(tl.sum(v_14, 1), tl.float32)
        # src[test_indexing.py:N]: acc = acc * alpha[:, None]
        subscript_1 = v_16[:, None]
        v_17 = acc_copy_0 * subscript_1
        # src[test_indexing.py:N]: p = p.to(v.dtype)
        v_18 = tl.cast(v_14, tl.bfloat16)
        # src[test_indexing.py:N]: acc = hl.dot(p, v_j, acc=acc)
        acc = tl.dot(tl.cast(v_18, tl.bfloat16), tl.cast(v_j, tl.bfloat16), acc=v_17, input_precision='tf32', out_dtype=tl.float32)
        # src[test_indexing.py:N]: l_i = l_i * alpha + l_ij
        v_19 = v_3_copy_0 * v_16
        v_3 = v_19 + l_ij
        # src[test_indexing.py:N]: m_i = m_ij
        v_1 = v_10
    # src[test_indexing.py:N]: m_i += torch.log2(l_i)
    v_21 = libdevice.log2(v_3)
    v_22 = v_1 + v_21
    # src[test_indexing.py:N]: acc = acc / l_i[:, None]
    subscript_2 = v_3[:, None]
    v_23 = acc / subscript_2
    # src[test_indexing.py:N]: lse[tile_m] = m_i
    tl.store(lse + indices_0 * 1, v_22, None)
    # src[test_indexing.py:N]: o[tile_m, :] = acc
    v_24 = tl.cast(v_23, tl.bfloat16)
    tl.store(o + (indices_0[:, None] * 64 + indices_3[None, :] * 1), v_24, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: B, H, M, D = q_in.shape
    B, H, M, D = q_in.shape
    # src[test_indexing.py:N]: Bk, Hk, N, Dk = k_in.shape
    Bk, Hk, N, Dk = k_in.shape
    # src[test_indexing.py:N]: Bv, Hv, Nv, Dv = v_in.shape
    Bv, Hv, Nv, Dv = v_in.shape
    # src[test_indexing.py:N]: D = hl.specialize(D)
    D = 64
    # src[test_indexing.py:N]: Dv = hl.specialize(Dv)
    Dv = 64
    # src[test_indexing.py:N]: q = q_in.reshape(-1, D)
    q = q_in.reshape(-1, D)
    # src[test_indexing.py:N]: k = k_in.reshape(-1, D)
    k = k_in.reshape(-1, D)
    # src[test_indexing.py:N]: v = v_in.reshape(-1, Dv)
    v = v_in.reshape(-1, Dv)
    # src[test_indexing.py:N]: MM = q.shape[0]
    MM = q.shape[0]
    # src[test_indexing.py:N]: o = q.new_empty(MM, Dv)
    o = q.new_empty(MM, Dv)
    # src[test_indexing.py:N]: lse = q.new_empty(MM, dtype=torch.float32)
    lse = q.new_empty(MM, dtype=torch.float32)
    # src[test_indexing.py:N]: for tile_m in hl.tile(MM, block_size=block_m):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_2 = 64
    # src[test_indexing.py:N]: for tile_n in hl.tile(0, N, block_size=block_n):
    # src[test_indexing.py:N]:     k_j = k[tile_n + start_N, :]
    # src[test_indexing.py:N]:     v_j = v[tile_n + start_N, :]
    # src[test_indexing.py:N-N]: ...
    _BLOCK_SIZE_1 = 32
    # src[test_indexing.py:N]: for tile_m in hl.tile(MM, block_size=block_m):
    # src[test_indexing.py:N]:     m_i = hl.zeros([tile_m]) - float("inf")
    # src[test_indexing.py:N]:     l_i = hl.zeros([tile_m]) + 1.0
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_attention, (triton.cdiv(8192, _BLOCK_SIZE_0),), q, k, v, lse, o, _BLOCK_SIZE_0, _RDIM_SIZE_2, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return o.reshape(B, H, M, Dv), lse.reshape(B, H, M)
    return (o.reshape(B, H, M, Dv), lse.reshape(B, H, M))

--- assertExpectedJournal(TestIndexing.test_tile_with_offset_pointer)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_tile_offset_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 190
    # src[test_indexing.py:N]: tile_offset = tile + 10
    v_0 = tl.full([], 10, tl.int32)
    v_1 = indices_0 + v_0
    # src[test_indexing.py:N]: out[tile] = x[tile_offset]
    load = tl.load(x + (indices_0 + 10) * 1, mask_0, other=0)
    tl.store(out + indices_0 * 1, load, mask_0)

def tile_offset_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: out = x.new_empty(x.size(0) - 10)
    out = x.new_empty(x.size(0) - 10)
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    _BLOCK_SIZE_0 = 32
    # src[test_indexing.py:N]: for tile in hl.tile(out.size(0)):
    # src[test_indexing.py:N]:     # Use tile + offset pattern
    # src[test_indexing.py:N]:     tile_offset = tile + 10
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_tile_offset_kernel, (triton.cdiv(190, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out

--- assertExpectedJournal(TestIndexing.test_tile_with_offset_tensor_descriptor)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

# src[test_indexing.py:N]: def tile_offset_2d_kernel(x: torch.Tensor) -> torch.Tensor:
# src[test_indexing.py:N]:     M, N = x.size()
# src[test_indexing.py:N]:     out = x.new_empty(M - 10, N)
# src[test_indexing.py:N-N]: ...
helion.runtime.set_triton_allocator()

@triton.jit
def _helion_tile_offset_2d_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    # src[test_indexing.py:N]: out[tile_m, :] = x[tile_offset, :]
    x_desc = tl.make_tensor_descriptor(x, [128, 64], [64, 1], [_BLOCK_SIZE_0, _RDIM_SIZE_1])
    out_desc = tl.make_tensor_descriptor(out, [118, 64], [64, 1], [_BLOCK_SIZE_0, _RDIM_SIZE_1])
    # src[test_indexing.py:N]: for tile_m in hl.tile(out.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_indexing.py:N]: tile_offset = tile_m + 10
    v_0 = tl.full([], 10, tl.int32)
    v_1 = indices_0 + v_0
    # src[test_indexing.py:N]: out[tile_m, :] = x[tile_offset, :]
    load = x_desc.load([offset_0 + 10, 0])
    out_desc.store([offset_0, 0], load)

def tile_offset_2d_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_indexing.py:N]: M, N = x.size()
    M, N = x.size()
    # src[test_indexing.py:N]: out = x.new_empty(M - 10, N)
    out = x.new_empty(M - 10, N)
    # src[test_indexing.py:N]: for tile_m in hl.tile(out.size(0)):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    # src[test_indexing.py:N]: for tile_m in hl.tile(out.size(0)):
    # src[test_indexing.py:N]:     # Use tile + offset pattern
    # src[test_indexing.py:N]:     tile_offset = tile_m + 10
    # src[test_indexing.py:N-N]: ...
    _launcher(_helion_tile_offset_2d_kernel, (triton.cdiv(118, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_indexing.py:N]: return out
    return out
