This file is automatically generated by assertExpectedJournal calls in test_rng.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestRNG.test_multiple_rng_ops)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_multiple_rng_ops_kernel(rand1, rand2, uniform, normal, randn_a, randn_b, randn_c, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, rng_seed_buffer):
    # src[test_rng.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_rng.py:N]: rand1[tile_m, tile_n] = torch.rand_like(x[tile_m, tile_n])
    rand = tl.rand(tl.load(rng_seed_buffer + 0), indices_0[:, None] * 64 + indices_1[None, :]).to(tl.float32)
    tl.store(rand1 + (indices_0[:, None] * 64 + indices_1[None, :] * 1), rand, None)
    # src[test_rng.py:N]: rand2[tile_m, tile_n] = torch.rand_like(x[tile_m, tile_n])
    rand_1 = tl.rand(tl.load(rng_seed_buffer + 1), indices_0[:, None] * 64 + indices_1[None, :]).to(tl.float32)
    tl.store(rand2 + (indices_0[:, None] * 64 + indices_1[None, :] * 1), rand_1, None)
    # src[test_rng.py:N]: uniform[tile_m, tile_n] = torch.rand_like(x[tile_m, tile_n])
    rand_2 = tl.rand(tl.load(rng_seed_buffer + 2), indices_0[:, None] * 64 + indices_1[None, :]).to(tl.float32)
    tl.store(uniform + (indices_0[:, None] * 64 + indices_1[None, :] * 1), rand_2, None)
    # src[test_rng.py:N]: normal[tile_m, tile_n] = torch.randn_like(x[tile_m, tile_n])
    randn = tl.randn(tl.load(rng_seed_buffer + 3), indices_0[:, None] * 64 + indices_1[None, :]).to(tl.float32)
    tl.store(normal + (indices_0[:, None] * 64 + indices_1[None, :] * 1), randn, None)
    # src[test_rng.py:N]: randn_a[tile_m, tile_n] = torch.randn_like(x[tile_m, tile_n])
    randn_1 = tl.randn(tl.load(rng_seed_buffer + 4), indices_0[:, None] * 64 + indices_1[None, :]).to(tl.float32)
    tl.store(randn_a + (indices_0[:, None] * 64 + indices_1[None, :] * 1), randn_1, None)
    # src[test_rng.py:N]: randn_b[tile_m, tile_n] = torch.randn_like(x[tile_m, tile_n])
    randn_2 = tl.randn(tl.load(rng_seed_buffer + 5), indices_0[:, None] * 64 + indices_1[None, :]).to(tl.float32)
    tl.store(randn_b + (indices_0[:, None] * 64 + indices_1[None, :] * 1), randn_2, None)
    # src[test_rng.py:N]: randn_c[tile_m, tile_n] = torch.randn_like(x[tile_m, tile_n])
    randn_3 = tl.randn(tl.load(rng_seed_buffer + 6), indices_0[:, None] * 64 + indices_1[None, :]).to(tl.float32)
    tl.store(randn_c + (indices_0[:, None] * 64 + indices_1[None, :] * 1), randn_3, None)

def multiple_rng_ops_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    from torch._inductor import inductor_prims
    # src[test_rng.py:N]: def multiple_rng_ops_kernel(
    # src[test_rng.py:N]:     x: torch.Tensor,
    # src[test_rng.py:N]: ) -> tuple[
    # src[test_rng.py:N-N]: ...
    _rng_seed_buffer = inductor_prims.seeds(7, torch.accelerator.current_accelerator())
    # src[test_rng.py:N]: rand1 = torch.zeros_like(x)
    rand1 = torch.zeros_like(x)
    # src[test_rng.py:N]: rand2 = torch.zeros_like(x)
    rand2 = torch.zeros_like(x)
    # src[test_rng.py:N]: uniform = torch.zeros_like(x)
    uniform = torch.zeros_like(x)
    # src[test_rng.py:N]: normal = torch.zeros_like(x)
    normal = torch.zeros_like(x)
    # src[test_rng.py:N]: randn_a = torch.zeros_like(x)
    randn_a = torch.zeros_like(x)
    # src[test_rng.py:N]: randn_b = torch.zeros_like(x)
    randn_b = torch.zeros_like(x)
    # src[test_rng.py:N]: randn_c = torch.zeros_like(x)
    randn_c = torch.zeros_like(x)
    # src[test_rng.py:N]: m, n = x.shape
    m, n = x.shape
    # src[test_rng.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    # src[test_rng.py:N]: for tile_m, tile_n in hl.tile([m, n]):
    # src[test_rng.py:N]:     # Two independent rand operations
    # src[test_rng.py:N]:     rand1[tile_m, tile_n] = torch.rand_like(x[tile_m, tile_n])
    # src[test_rng.py:N-N]: ...
    _launcher(_helion_multiple_rng_ops_kernel, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1),), rand1, rand2, uniform, normal, randn_a, randn_b, randn_c, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _rng_seed_buffer, num_warps=4, num_stages=1)
    # src[test_rng.py:N]: randn_sum = randn_a + randn_b + randn_c
    randn_sum = randn_a + randn_b + randn_c
    # src[test_rng.py:N]: return rand1, rand2, uniform, normal, randn_sum
    return (rand1, rand2, uniform, normal, randn_sum)

--- assertExpectedJournal(TestRNG.test_rand_like_nested_tiles_issue_1208)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_nested_tiles_rand(q, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr, rng_seed_buffer):
    # src[test_rng.py:N]: for tile_b, tile_q in hl.tile([B, T]):
    num_blocks_0 = tl.cdiv(2, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_2 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[test_rng.py:N]: qs = q[tile_b, tile_q, :]
    qs = tl.load(q + (indices_0[:, None, None] * 512 + indices_1[None, :, None] * 32 + indices_2[None, None, :] * 1), None)
    # src[test_rng.py:N]: for tile_k in hl.tile(T):
    # src[test_rng.py:N]:     ks = q[tile_b, tile_k, :]
    # src[test_rng.py:N]:     # logits has shape [tile_b, tile_q, tile_k]
    # src[test_rng.py:N-N]: ...
    for offset_3 in tl.range(0, 16, _BLOCK_SIZE_3):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        qs_copy = qs
        qs_copy_0 = qs_copy
        # src[test_rng.py:N]: ks = q[tile_b, tile_k, :]
        ks = tl.load(q + (indices_0[:, None, None] * 512 + indices_3[None, :, None] * 32 + indices_2[None, None, :] * 1), None)
        # src[test_rng.py:N]: logits = qs @ ks.transpose(-1, -2)
        permute = tl.permute(ks, [0, 2, 1])
        logits = tl.dot(tl.cast(qs_copy_0, tl.float32), tl.cast(permute, tl.float32), input_precision='tf32', out_dtype=tl.float32)
        # src[test_rng.py:N]: rand = torch.rand_like(logits)
        rand = tl.rand(tl.load(rng_seed_buffer + 0), indices_0[:, None, None] * 16 * 16 + indices_1[None, :, None] * 16 + indices_3[None, None, :]).to(tl.float32)
        # src[test_rng.py:N]: mask = ((logits + rand) > 0).float()
        v_0 = logits + rand
        v_1 = 0.0
        v_2 = v_0 > v_1
        v_3 = tl.cast(v_2, tl.float32)
        # src[test_rng.py:N]: out[tile_b, tile_q, :] = torch.matmul(mask, q[tile_b, tile_q, :])
        load_1 = tl.load(q + (indices_0[:, None, None] * 512 + indices_1[None, :, None] * 32 + indices_2[None, None, :] * 1), None)
        bmm_1 = tl.dot(tl.cast(v_3, tl.float32), tl.cast(load_1, tl.float32), input_precision='tf32', out_dtype=tl.float32)
        tl.store(out + (indices_0[:, None, None] * 512 + indices_1[None, :, None] * 32 + indices_2[None, None, :] * 1), bmm_1, None)

def nested_tiles_rand(q: torch.Tensor, *, _launcher=_default_launcher):
    from torch._inductor import inductor_prims
    # src[test_rng.py:N]: def nested_tiles_rand(q: torch.Tensor) -> torch.Tensor:
    # src[test_rng.py:N]:     B, T, H = q.shape
    # src[test_rng.py:N]:     out = torch.empty((B, T, H), device=q.device, dtype=q.dtype)
    # src[test_rng.py:N-N]: ...
    _rng_seed_buffer = inductor_prims.seeds(1, torch.accelerator.current_accelerator())
    # src[test_rng.py:N]: B, T, H = q.shape
    B, T, H = q.shape
    # src[test_rng.py:N]: out = torch.empty((B, T, H), device=q.device, dtype=q.dtype)
    out = torch.empty((B, T, H), device=q.device, dtype=q.dtype)
    # src[test_rng.py:N]: for tile_b, tile_q in hl.tile([B, T]):
    _BLOCK_SIZE_0 = 2
    _BLOCK_SIZE_1 = 16
    _RDIM_SIZE_2 = 32
    # src[test_rng.py:N]: for tile_k in hl.tile(T):
    # src[test_rng.py:N]:     ks = q[tile_b, tile_k, :]
    # src[test_rng.py:N]:     # logits has shape [tile_b, tile_q, tile_k]
    # src[test_rng.py:N-N]: ...
    _BLOCK_SIZE_3 = 16
    # src[test_rng.py:N]: for tile_b, tile_q in hl.tile([B, T]):
    # src[test_rng.py:N]:     qs = q[tile_b, tile_q, :]
    # src[test_rng.py:N]:     for tile_k in hl.tile(T):
    # src[test_rng.py:N-N]: ...
    _launcher(_helion_nested_tiles_rand, (triton.cdiv(2, _BLOCK_SIZE_0) * triton.cdiv(16, _BLOCK_SIZE_1),), q, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, _rng_seed_buffer, num_warps=4, num_stages=1)
    # src[test_rng.py:N]: return out
    return out

--- assertExpectedJournal(TestRNG.test_rand_like_with_specialized_dimension)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_with_rand(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, rng_seed_buffer):
    # src[test_rng.py:N]: for tile_m in hl.tile(m):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_3 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    # src[test_rng.py:N]: acc = hl.zeros([tile_m, n], dtype=torch.float32)
    acc = tl.full([_BLOCK_SIZE_0, 64], 0.0, tl.float32)
    # src[test_rng.py:N]: for tile_k in hl.tile(k):
    # src[test_rng.py:N]:     mm = torch.matmul(x[tile_m, tile_k], y[tile_k, :])
    # src[test_rng.py:N]:     acc = acc + mm
    for offset_2 in tl.range(0, 512, _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        # src[test_rng.py:N]: mm = torch.matmul(x[tile_m, tile_k], y[tile_k, :])
        load = tl.load(x + (indices_0[:, None] * 512 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 64 + indices_3[None, :] * 1), None)
        mm = tl.cast(tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), input_precision='tf32', out_dtype=tl.float32), tl.float16)
        # src[test_rng.py:N]: acc = acc + mm
        v_0 = tl.cast(mm, tl.float32)
        acc = acc_copy_0 + v_0
    # src[test_rng.py:N]: noise = torch.rand_like(acc, dtype=torch.float32)
    noise = tl.rand(tl.load(rng_seed_buffer + 0), indices_0[:, None] * 64 + tl.arange(0, 64)[None, :]).to(tl.float32)
    # src[test_rng.py:N]: acc = acc + noise * 0.01  # Small noise
    v_2 = 0.01
    v_3 = noise * v_2
    v_4 = acc + v_3
    # src[test_rng.py:N]: out[tile_m, :] = acc.to(out.dtype)
    v_5 = tl.cast(v_4, tl.float16)
    tl.store(out + (indices_0[:, None] * 64 + indices_3[None, :] * 1), v_5, None)

def matmul_with_rand(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    from torch._inductor import inductor_prims
    # src[test_rng.py:N]: def matmul_with_rand(
    # src[test_rng.py:N]:     x: torch.Tensor,
    # src[test_rng.py:N]:     y: torch.Tensor,
    # src[test_rng.py:N-N]: ...
    _rng_seed_buffer = inductor_prims.seeds(1, torch.accelerator.current_accelerator())
    # src[test_rng.py:N]: m, k = x.size()
    m, k = x.size()
    # src[test_rng.py:N]: k2, n = y.size()
    k2, n = y.size()
    # src[test_rng.py:N]: n = hl.specialize(n)
    n = 64
    # src[test_rng.py:N]: out = torch.empty(
    # src[test_rng.py:N]:     [m, n],
    # src[test_rng.py:N]:     dtype=torch.promote_types(x.dtype, y.dtype),
    # src[test_rng.py:N-N]: ...
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    # src[test_rng.py:N]: for tile_m in hl.tile(m):
    _BLOCK_SIZE_0 = 64
    _RDIM_SIZE_2 = 64
    # src[test_rng.py:N]: for tile_k in hl.tile(k):
    # src[test_rng.py:N]:     mm = torch.matmul(x[tile_m, tile_k], y[tile_k, :])
    # src[test_rng.py:N]:     acc = acc + mm
    _BLOCK_SIZE_1 = 128
    # src[test_rng.py:N]: for tile_m in hl.tile(m):
    # src[test_rng.py:N]:     acc = hl.zeros([tile_m, n], dtype=torch.float32)
    # src[test_rng.py:N]:     for tile_k in hl.tile(k):
    # src[test_rng.py:N-N]: ...
    _launcher(_helion_matmul_with_rand, (triton.cdiv(256, _BLOCK_SIZE_0),), x, y, out, _BLOCK_SIZE_0, _RDIM_SIZE_2, _BLOCK_SIZE_1, _rng_seed_buffer, num_warps=4, num_stages=1)
    # src[test_rng.py:N]: return out
    return out
