This file is automatically generated by assertExpectedJournal calls in test_views.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestViews.test_reshape_sum)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fn(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _SHAPE_DIM: tl.constexpr):
    # src[test_views.py:N]: for tile0 in hl.tile(x.size(0)):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    # src[test_views.py:N]: acc = hl.zeros([tile0], dtype=x.dtype)
    acc = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    # src[test_views.py:N]: for tile1, tile2 in hl.tile([x.size(1), x.size(2)]):
    # src[test_views.py:N]:     acc += x[tile0, tile1, tile2].reshape(tile0, -1).sum(-1)
    for offset_3 in tl.range(0, 4, _BLOCK_SIZE_1):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        for offset_4 in tl.range(0, 5, _BLOCK_SIZE_2):
            indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_4 < 5
            acc_copy = acc
            acc_copy_0 = acc_copy
            # src[test_views.py:N]: acc += x[tile0, tile1, tile2].reshape(tile0, -1).sum(-1)
            load = tl.load(x + (indices_0[:, None, None] * 20 + indices_3[None, :, None] * 5 + indices_4[None, None, :] * 1), mask_0[:, None, None] & mask_2[None, None, :], other=0)
            view = tl.reshape(load, [_BLOCK_SIZE_0, _SHAPE_DIM])
            sum_1 = tl.cast(tl.sum(view, 1), tl.float32)
            acc = acc_copy_0 + sum_1
    # src[test_views.py:N]: out[tile0] = acc
    tl.store(out + indices_0 * 1, acc, mask_0)

def fn(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_views.py:N]: out = x.new_empty([x.size(0)])
    out = x.new_empty([x.size(0)])
    # src[test_views.py:N]: for tile0 in hl.tile(x.size(0)):
    _BLOCK_SIZE_0 = 4
    # src[test_views.py:N]: for tile1, tile2 in hl.tile([x.size(1), x.size(2)]):
    # src[test_views.py:N]:     acc += x[tile0, tile1, tile2].reshape(tile0, -1).sum(-1)
    _BLOCK_SIZE_2 = 8
    _BLOCK_SIZE_1 = 4
    # src[test_views.py:N]: acc += x[tile0, tile1, tile2].reshape(tile0, -1).sum(-1)
    _SHAPE_DIM = _BLOCK_SIZE_1 * _BLOCK_SIZE_2
    # src[test_views.py:N]: for tile0 in hl.tile(x.size(0)):
    # src[test_views.py:N]:     acc = hl.zeros([tile0], dtype=x.dtype)
    # src[test_views.py:N]:     for tile1, tile2 in hl.tile([x.size(1), x.size(2)]):
    # src[test_views.py:N-N]: ...
    _RDIM_SIZE_3 = triton.next_power_of_2(_BLOCK_SIZE_1 * _BLOCK_SIZE_2)
    _launcher(_helion_fn, (triton.cdiv(3, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_2, _BLOCK_SIZE_1, _SHAPE_DIM, num_warps=4, num_stages=1)
    # src[test_views.py:N]: return out
    return out

--- assertExpectedJournal(TestViews.test_softmax_unsqueeze)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, _RDIM_SIZE_1: tl.constexpr):
    # src[test_views.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_views.py:N]: values = x[tile_n, :]
    values = tl.load(x + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), None)
    # src[test_views.py:N]: amax = torch.amax(values, dim=1).unsqueeze(1)
    amax = tl.cast(tl.max(values, 1), tl.float16)
    amax_1 = amax[:, None]
    # src[test_views.py:N]: exp = torch.exp(values - amax)
    v_0 = values - amax_1
    v_1 = tl.cast(v_0, tl.float32)
    v_2 = libdevice.exp(v_1)
    v_3 = tl.cast(v_2, tl.float16)
    # src[test_views.py:N]: sum_exp = torch.unsqueeze(torch.sum(exp, dim=1), -1)
    sum_1 = tl.cast(tl.sum(v_3, 1), tl.float16)
    sum_exp = sum_1[None, :]
    # src[test_views.py:N]: out[tile_n, :] = exp / sum_exp
    v_4 = v_3 / sum_exp
    tl.store(out + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), v_4, None)

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_views.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_views.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_views.py:N]: for tile_n in hl.tile(n):
    _RDIM_SIZE_1 = 1024
    # src[test_views.py:N]: for tile_n in hl.tile(n):
    # src[test_views.py:N]:     values = x[tile_n, :]
    # src[test_views.py:N]:     amax = torch.amax(values, dim=1).unsqueeze(1)
    # src[test_views.py:N-N]: ...
    _launcher(_helion_softmax, (1024,), x, out, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    # src[test_views.py:N]: return out
    return out

--- assertExpectedJournal(TestViews.test_softmax_view_reshape)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, _RDIM_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_views.py:N]: for tile_n in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    # src[test_views.py:N]: values = x[tile_n, :]
    values = tl.load(x + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), None)
    # src[test_views.py:N]: amax = torch.amax(values, dim=1).view(tile_n, 1)
    amax = tl.cast(tl.max(values, 1), tl.float16)
    amax_1 = tl.reshape(amax, [_BLOCK_SIZE_0, 1])
    # src[test_views.py:N]: exp = torch.exp(values - amax)
    v_0 = values - amax_1
    v_1 = tl.cast(v_0, tl.float32)
    v_2 = libdevice.exp(v_1)
    v_3 = tl.cast(v_2, tl.float16)
    # src[test_views.py:N]: sum_exp = torch.reshape(torch.sum(exp, dim=1), [tile_n, 1])
    sum_1 = tl.cast(tl.sum(v_3, 1), tl.float16)
    sum_exp = tl.reshape(sum_1, [_BLOCK_SIZE_0, 1])
    # src[test_views.py:N]: out[tile_n, :] = exp / sum_exp
    v_4 = v_3 / sum_exp
    tl.store(out + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), v_4, None)

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_views.py:N]: n, _m = x.size()
    n, _m = x.size()
    # src[test_views.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_views.py:N]: for tile_n in hl.tile(n):
    _RDIM_SIZE_1 = 1024
    # src[test_views.py:N]: values = x[tile_n, :]
    _BLOCK_SIZE_0 = 1
    # src[test_views.py:N]: for tile_n in hl.tile(n):
    # src[test_views.py:N]:     values = x[tile_n, :]
    # src[test_views.py:N]:     amax = torch.amax(values, dim=1).view(tile_n, 1)
    # src[test_views.py:N-N]: ...
    _launcher(_helion_softmax, (1024,), x, out, _RDIM_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_views.py:N]: return out
    return out

--- assertExpectedJournal(TestViews.test_specialize_reshape)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fn(reshaped, out, _BLOCK_SIZE_2: tl.constexpr):
    # src[test_views.py:N]: for tile in hl.tile(reshaped.size()):
    num_blocks_0 = 2
    num_blocks_1 = 3
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1
    indices_1 = offset_1 + tl.zeros([1], tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    # src[test_views.py:N]: out[tile] = reshaped[tile] + 1
    load = tl.load(reshaped + (indices_0[:, None, None] * 96 + indices_1[None, :, None] * 32 + indices_2[None, None, :] * 1), None)
    v_0 = 1.0
    v_1 = load + v_0
    tl.store(out + (indices_0[:, None, None] * 96 + indices_1[None, :, None] * 32 + indices_2[None, None, :] * 1), v_1, None)

def fn(x: torch.Tensor, chunk_size: int, *, _launcher=_default_launcher):
    # src[test_views.py:N]: batch, seqlen = x.shape
    batch, seqlen = x.shape
    # src[test_views.py:N]: chunk_size = hl.specialize(chunk_size)
    chunk_size = 32
    # src[test_views.py:N]: nchunks = (seqlen + chunk_size - 1) // chunk_size
    nchunks = (seqlen + chunk_size - 1) // chunk_size
    # src[test_views.py:N]: reshaped = x.reshape(batch, nchunks, chunk_size)
    reshaped = x.reshape(batch, nchunks, chunk_size)
    # src[test_views.py:N]: out = torch.empty_like(reshaped)
    out = torch.empty_like(reshaped)
    # src[test_views.py:N]: for tile in hl.tile(reshaped.size()):
    _BLOCK_SIZE_2 = 32
    # src[test_views.py:N]: for tile in hl.tile(reshaped.size()):
    # src[test_views.py:N]:     out[tile] = reshaped[tile] + 1
    _launcher(_helion_fn, (2 * 3 * triton.cdiv(32, _BLOCK_SIZE_2),), reshaped, out, _BLOCK_SIZE_2, num_warps=4, num_stages=1)
    # src[test_views.py:N]: return out.reshape(batch, seqlen)
    return out.reshape(batch, seqlen)

--- assertExpectedJournal(TestViews.test_squeeze)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

# src[test_views.py:N]: def fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
# src[test_views.py:N]:     out = torch.empty_like(x)
# src[test_views.py:N]:     for tile_n, tile_m in hl.tile(x.size()):
# src[test_views.py:N-N]: ...
helion.runtime.set_triton_allocator()

@triton.jit
def _helion_fn(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_views.py:N]: out[tile_n, tile_m] = x[tile_n, tile_m] + y[tile_m, :].squeeze(
    x_desc = tl.make_tensor_descriptor(x, [1024, 1024], [1024, 1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    # src[test_views.py:N]: out[tile_n, tile_m] = x[tile_n, tile_m] + y[tile_m, :].squeeze(
    # src[test_views.py:N]:     1
    # src[test_views.py:N]: ).unsqueeze(0)
    out_desc = tl.make_tensor_descriptor(out, [1024, 1024], [1024, 1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    # src[test_views.py:N]: for tile_n, tile_m in hl.tile(x.size()):
    num_blocks_0 = tl.cdiv(1024, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    # src[test_views.py:N]: out[tile_n, tile_m] = x[tile_n, tile_m] + y[tile_m, :].squeeze(
    load = x_desc.load([offset_0, offset_1])
    load_1 = tl.load(y + indices_1[:, None] * 1, None)
    # src[test_views.py:N]: out[tile_n, tile_m] = x[tile_n, tile_m] + y[tile_m, :].squeeze(
    # src[test_views.py:N]:     1
    # src[test_views.py:N]: ).unsqueeze(0)
    squeeze = tl.reshape(load_1, [_BLOCK_SIZE_1])
    unsqueeze = squeeze[None, :]
    v_0 = load + unsqueeze
    out_desc.store([offset_0, offset_1], v_0)

def fn(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_views.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_views.py:N]: for tile_n, tile_m in hl.tile(x.size()):
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    # src[test_views.py:N]: for tile_n, tile_m in hl.tile(x.size()):
    # src[test_views.py:N]:     out[tile_n, tile_m] = x[tile_n, tile_m] + y[tile_m, :].squeeze(
    # src[test_views.py:N]:         1
    # src[test_views.py:N-N]: ...
    _launcher(_helion_fn, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_views.py:N]: return out
    return out

--- assertExpectedJournal(TestViews.test_stack_dim0)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test_stack_dim0_kernel(a, b, c, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_views.py:N]: for tile_m in hl.tile(M):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 65
    indices_3 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_3 < 3
    # src[test_views.py:N]: for tile_n in hl.tile(N):
    # src[test_views.py:N]:     a_tile = a[tile_m, tile_n]
    # src[test_views.py:N]:     b_tile = b[tile_m, tile_n]
    # src[test_views.py:N-N]: ...
    for offset_2 in tl.range(0, 129, _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_2 < 129
        # src[test_views.py:N]: a_tile = a[tile_m, tile_n]
        a_tile = tl.load(a + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        # src[test_views.py:N]: b_tile = b[tile_m, tile_n]
        b_tile = tl.load(b + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        # src[test_views.py:N]: c_tile = c[tile_m, tile_n]
        c_tile = tl.load(c + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        # src[test_views.py:N]: stacked = torch.stack([a_tile, b_tile, c_tile], dim=0)
        stack_idx = tl.arange(0, 4)
        broadcast_idx = stack_idx[:, None, None]
        expanded_0 = tl.expand_dims(a_tile, 0)
        expanded_1 = tl.expand_dims(b_tile, 0)
        expanded_2 = tl.expand_dims(c_tile, 0)
        stacked_result = tl.zeros_like(expanded_0)
        mask_3 = broadcast_idx == 0
        stacked_result = tl.where(mask_3, expanded_0, stacked_result)
        mask_4 = broadcast_idx == 1
        stacked_result = tl.where(mask_4, expanded_1, stacked_result)
        mask_5 = broadcast_idx == 2
        stacked_result = tl.where(mask_5, expanded_2, stacked_result)
        # src[test_views.py:N]: result[:, tile_m, tile_n] = stacked
        tl.store(result + (indices_3[:, None, None] * 8385 + indices_0[None, :, None] * 129 + indices_2[None, None, :] * 1), stacked_result, mask_2[:, None, None] & mask_0[None, :, None] & mask_1[None, None, :])

def test_stack_dim0_kernel(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_views.py:N]: M, N = a.shape
    M, N = a.shape
    # src[test_views.py:N]: result = torch.zeros(3, M, N, dtype=a.dtype, device=a.device)
    result = torch.zeros(3, M, N, dtype=a.dtype, device=a.device)
    # src[test_views.py:N]: for tile_m in hl.tile(M):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_2 = 4
    # src[test_views.py:N]: for tile_n in hl.tile(N):
    # src[test_views.py:N]:     a_tile = a[tile_m, tile_n]
    # src[test_views.py:N]:     b_tile = b[tile_m, tile_n]
    # src[test_views.py:N-N]: ...
    _BLOCK_SIZE_1 = 32
    # src[test_views.py:N]: for tile_m in hl.tile(M):
    # src[test_views.py:N]:     for tile_n in hl.tile(N):
    # src[test_views.py:N]:         a_tile = a[tile_m, tile_n]
    # src[test_views.py:N-N]: ...
    _launcher(_helion_test_stack_dim0_kernel, (triton.cdiv(65, _BLOCK_SIZE_0),), a, b, c, result, _BLOCK_SIZE_0, _RDIM_SIZE_2, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_views.py:N]: return result
    return result

--- assertExpectedJournal(TestViews.test_stack_non_power_of_2)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test_stack_non_power_of_2_kernel(a, b, c, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    # src[test_views.py:N]: for tile_m in hl.tile(M):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 65
    indices_3 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_3 < 3
    # src[test_views.py:N]: for tile_n in hl.tile(N):
    # src[test_views.py:N]:     a_tile = a[tile_m, tile_n]
    # src[test_views.py:N]:     b_tile = b[tile_m, tile_n]
    # src[test_views.py:N-N]: ...
    for offset_2 in tl.range(0, 129, _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_2 < 129
        # src[test_views.py:N]: a_tile = a[tile_m, tile_n]
        a_tile = tl.load(a + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        # src[test_views.py:N]: b_tile = b[tile_m, tile_n]
        b_tile = tl.load(b + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        # src[test_views.py:N]: c_tile = c[tile_m, tile_n]
        c_tile = tl.load(c + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        # src[test_views.py:N]: stacked = torch.stack([a_tile, b_tile, c_tile], dim=1)
        stack_idx = tl.arange(0, 4)
        broadcast_idx = stack_idx[None, :, None]
        expanded_0 = tl.expand_dims(a_tile, 1)
        expanded_1 = tl.expand_dims(b_tile, 1)
        expanded_2 = tl.expand_dims(c_tile, 1)
        stacked_result = tl.zeros_like(expanded_0)
        mask_3 = broadcast_idx == 0
        stacked_result = tl.where(mask_3, expanded_0, stacked_result)
        mask_4 = broadcast_idx == 1
        stacked_result = tl.where(mask_4, expanded_1, stacked_result)
        mask_5 = broadcast_idx == 2
        stacked_result = tl.where(mask_5, expanded_2, stacked_result)
        # src[test_views.py:N]: result[tile_m, :, tile_n] = stacked
        tl.store(result + (indices_0[:, None, None] * 387 + indices_3[None, :, None] * 129 + indices_2[None, None, :] * 1), stacked_result, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :])

def test_stack_non_power_of_2_kernel(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_views.py:N]: M, N = a.shape
    M, N = a.shape
    # src[test_views.py:N]: result = torch.zeros(M, 3, N, dtype=a.dtype, device=a.device)
    result = torch.zeros(M, 3, N, dtype=a.dtype, device=a.device)
    # src[test_views.py:N]: for tile_m in hl.tile(M):
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_2 = 4
    # src[test_views.py:N]: for tile_n in hl.tile(N):
    # src[test_views.py:N]:     a_tile = a[tile_m, tile_n]
    # src[test_views.py:N]:     b_tile = b[tile_m, tile_n]
    # src[test_views.py:N-N]: ...
    _BLOCK_SIZE_1 = 32
    # src[test_views.py:N]: for tile_m in hl.tile(M):
    # src[test_views.py:N]:     for tile_n in hl.tile(N):
    # src[test_views.py:N]:         a_tile = a[tile_m, tile_n]
    # src[test_views.py:N-N]: ...
    _launcher(_helion_test_stack_non_power_of_2_kernel, (triton.cdiv(65, _BLOCK_SIZE_0),), a, b, c, result, _BLOCK_SIZE_0, _RDIM_SIZE_2, _BLOCK_SIZE_1, num_warps=4, num_stages=1)
    # src[test_views.py:N]: return result
    return result

--- assertExpectedJournal(TestViews.test_view_blocksize_constexpr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_foo(x, out, _BLOCK_SIZE_0: tl.constexpr, floordiv: tl.constexpr, _SHAPE_DIM: tl.constexpr):
    # src[test_views.py:N]: for (n_tile,) in hl.tile([N]):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_views.py:N]: val = x[n_tile]
    val = tl.load(x + indices_0 * 1, None)
    # src[test_views.py:N]: val = val.view(n_tile.block_size // 2, 2)
    val_1 = tl.reshape(val, [_SHAPE_DIM, 2])
    # src[test_views.py:N]: val_a, val_b = hl.split(val)
    val_a = tl.split(val_1)[0]
    val_b = tl.split(val_1)[1]
    # src[test_views.py:N]: out[n_tile.begin + hl.arange(0, n_tile.block_size // 2)] = val_a + val_b
    v_0 = val_a + val_b
    iota = tl.arange(0, floordiv)
    v_1 = tl.cast(offset_0, tl.int32)
    v_2 = iota + v_1
    tl.store(out + v_2 * 1, v_0, None)

def foo(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_views.py:N]: N = x.shape[0]
    N = x.shape[0]
    # src[test_views.py:N]: N = hl.specialize(N)
    N = 1024
    # src[test_views.py:N]: out = x.new_empty(N // 2)
    out = x.new_empty(N // 2)
    # src[test_views.py:N]: for (n_tile,) in hl.tile([N]):
    _BLOCK_SIZE_0 = 32
    # src[test_views.py:N]: val = val.view(n_tile.block_size // 2, 2)
    _SHAPE_DIM = _BLOCK_SIZE_0 // 2
    # src[test_views.py:N]: for (n_tile,) in hl.tile([N]):
    # src[test_views.py:N]:     val = x[n_tile]
    # src[test_views.py:N]:     val = val.view(n_tile.block_size // 2, 2)
    # src[test_views.py:N-N]: ...
    _launcher(_helion_foo, (triton.cdiv(1024, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_0 // 2, _SHAPE_DIM, num_warps=4, num_stages=1)
    # src[test_views.py:N]: return out
    return out

--- assertExpectedJournal(TestViews.test_view_dtype_reinterpret)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_view_dtype_kernel(x, out, _BLOCK_SIZE_0: tl.constexpr):
    # src[test_views.py:N]: for tile in hl.tile(n):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    # src[test_views.py:N]: val = x[tile]
    val = tl.load(x + indices_0 * 1, None)
    # src[test_views.py:N]: val_as_int = val.view(dtype=torch.int16)
    view = tl.cast(val, tl.int16, bitcast=True)
    # src[test_views.py:N]: val_as_int = val_as_int + 1
    v_0 = tl.full([], 1, tl.int16)
    v_1 = view + v_0
    # src[test_views.py:N]: val_back = val_as_int.view(dtype=torch.bfloat16)
    view_1 = tl.cast(v_1, tl.bfloat16, bitcast=True)
    # src[test_views.py:N]: out[tile] = val_back
    tl.store(out + indices_0 * 1, view_1, None)

def view_dtype_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    # src[test_views.py:N]: n = x.size(0)
    n = x.size(0)
    # src[test_views.py:N]: out = torch.empty_like(x)
    out = torch.empty_like(x)
    # src[test_views.py:N]: for tile in hl.tile(n):
    _BLOCK_SIZE_0 = 32
    # src[test_views.py:N]: for tile in hl.tile(n):
    # src[test_views.py:N]:     val = x[tile]
    # src[test_views.py:N]:     # View bf16 as int16, add 1 to raw bits, view back as bf16
    # src[test_views.py:N-N]: ...
    _launcher(_helion_view_dtype_kernel, (triton.cdiv(1024, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, num_warps=4, num_stages=1)
    # src[test_views.py:N]: return out
    return out
